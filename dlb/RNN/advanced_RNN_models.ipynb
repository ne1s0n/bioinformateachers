{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "16f2f29e",
      "metadata": {
        "id": "16f2f29e"
      },
      "source": [
        "# Advanced RNN (recurrent neural network) models\n",
        "\n",
        "In this notebook, we illustrate how to use RNN models to analyse time series (longitudinal) data.\n",
        "The specific type of statistical problem here is **forecasting**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries and setting the random seed\n",
        "\n",
        "First of all, we load some necessary libraries; then we setup the random seed to ensure reproducibility of results. Since tensorflow uses an internal random generator we need to fix both the general seed (via numpy `seed()`) and tensorflow seed (via `set_seet()`)"
      ],
      "metadata": {
        "id": "5Z63YlH0OgfH"
      },
      "id": "5Z63YlH0OgfH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e86c1bf",
      "metadata": {
        "id": "9e86c1bf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime as dt\n",
        "import pandas_datareader\n",
        "\n",
        "from pandas_datareader import data\n",
        "\n",
        "  # Set the seed using keras.utils.set_random_seed. This will set:\n",
        "  # 1) `numpy` seed\n",
        "  # 2) `tensorflow` random seed\n",
        "  # 3) `python` random seed\n",
        "tf.keras.utils.set_random_seed(10)\n",
        "\n",
        "  # This will make TensorFlow ops as deterministic as possible, but it will\n",
        "  # affect the overall performance, so it's not enabled by default.\n",
        "  # `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## A RNN model to analyse climate data\n",
        "\n",
        "RNN are a type of deep learning architecture suited to work with **longitudinal data**, e.g. time series data.\n",
        "\n",
        "Time series data include many types of data, like:\n",
        "\n",
        "- economic indicators (e.g. *quarterly GDP*, *monthly inflation*)\n",
        "- patient health evolution metrics (e.g.*ECG data*)\n",
        "\n",
        "Time series data include a fundamental **time component**, and are often used in **forecasting problems**.\n",
        "\n",
        "Here we use maximum daily temperatures in Melbourne (Australia), over the period 1981 - 1990, as an example to train a recurrent neural network (RNN) model."
      ],
      "metadata": {
        "id": "zZ3vzxq4B0IQ"
      },
      "id": "zZ3vzxq4B0IQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09318d92",
      "metadata": {
        "id": "09318d92"
      },
      "outputs": [],
      "source": [
        "## getting the data\n",
        "DATAURL = 'https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-max-temperatures.csv'\n",
        "df = pd.read_csv(DATAURL)\n",
        "print(\"N. of row in dataframe: \",len(df))\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will use Melbourne maximum daily temperature (`Temperature`) in a forecasting problem\n",
        "\n",
        "First, we split data into train/test: we use the last year as test, and all remaining observations as training set."
      ],
      "metadata": {
        "id": "AUAICkwSxATR"
      },
      "id": "AUAICkwSxATR"
    },
    {
      "cell_type": "code",
      "source": [
        "temperature = df['Temperature']\n",
        "n = len(temperature) - 365 # training set\n",
        "print('The size of the training data is', n)"
      ],
      "metadata": {
        "id": "8yxv4iLcxDJN"
      },
      "id": "8yxv4iLcxDJN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's have a look at the distribution of the training and testing data:"
      ],
      "metadata": {
        "id": "FCKrVNxlxFeh"
      },
      "id": "FCKrVNxlxFeh"
    },
    {
      "cell_type": "code",
      "source": [
        "df['Temperature'][:n].plot()"
      ],
      "metadata": {
        "id": "Hf-CrRaOxDhR"
      },
      "id": "Hf-CrRaOxDhR",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['Temperature'][n:].plot()"
      ],
      "metadata": {
        "id": "i0GTgclVxHfh"
      },
      "id": "i0GTgclVxHfh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train = list(temperature)[:n] #first n temperatures\n",
        "test = list(temperature)[n:] #remaining temperatures"
      ],
      "metadata": {
        "id": "jaXuQ9QDxK4p"
      },
      "id": "jaXuQ9QDxK4p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Data representation\n",
        "\n",
        "We have two series of data (temperatures), for training and for testing.\n",
        "For training, we have therefore one data series, and we need to use an appropriate representation for the feature data to be used for prediction (forecasting).\n",
        "\n",
        "A common possibility is to use preceding values in the series to predict the next one(s): for instance, a sliding-window appraoch can be used (see figure below)\n",
        "\n",
        "<img src=\"https://miro.medium.com/max/700/1*murkc0tNsvgdQDVgKqSdfQ.png\">"
      ],
      "metadata": {
        "id": "ZEmeBuqvxP6K"
      },
      "id": "ZEmeBuqvxP6K"
    },
    {
      "cell_type": "code",
      "source": [
        "## prepare the training data\n",
        "windowSize, X_train, y_train = 10, [], [] #initialize lists and set window size\n",
        "for index in range(len(train)-windowSize): #we must end at train-windowSize to avoid the windowSize going past the end\n",
        "    curr_value = train[index:index+windowSize]\n",
        "    #print(index)\n",
        "    #print(curr_value)\n",
        "    X_train.append(curr_value) #append the range from index to index+windowSize to x\n",
        "    y_train.append(train[index+windowSize]) #append the next value to the y\n",
        "\n",
        "X_train,y_train = np.array(X_train), np.array(y_train) #convert to numpy array\n",
        "X_train = X_train.reshape((len(X_train), windowSize, 1)) #reshape X_train to proper 3-d array\n",
        "\n",
        "print(y_train.shape)\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "fUAuhD04xOIB"
      },
      "id": "fUAuhD04xOIB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## prepare the test data\n",
        "windowSize, X_test, y_test = 10, [], [] #initialize lists and set window size\n",
        "for index in range(len(test)-windowSize): #we must end at train-windowSize to avoid the windowSize going past the end\n",
        "    X_test.append(test[index:index+windowSize]) #append the range from index to index+windowSize to x\n",
        "    y_test.append(test[index+windowSize]) #append the next value to the y\n",
        "\n",
        "X_test,y_test = np.array(X_test), np.array(y_test) #convert to numpy array\n",
        "X_test = X_test.reshape((len(X_test), windowSize, 1)) #reshape X_train to proper 3-d array\n",
        "\n",
        "print(y_test.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "jXc1hS8LxVa1"
      },
      "id": "jXc1hS8LxVa1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0,0:10,0]"
      ],
      "metadata": {
        "id": "LO_rVjayo00h"
      },
      "id": "LO_rVjayo00h",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "id": "PfQvUOwQpUw0"
      },
      "id": "PfQvUOwQpUw0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Simple RNN\n",
        "\n",
        "Let's start with a simple RNN model:\n",
        "- one RNN layer (12 units)\n",
        "- one dense layer (output layer: one number per timepoint, regression problem)"
      ],
      "metadata": {
        "id": "htXA9Xb0xY5-"
      },
      "id": "htXA9Xb0xY5-"
    },
    {
      "cell_type": "code",
      "source": [
        "from keras import Sequential\n",
        "from keras.layers import SimpleRNN, Dense, Input\n",
        "\n",
        "import math\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "wQ6ZjV9cxW8Y"
      },
      "id": "wQ6ZjV9cxW8Y",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " input_shape = (windowSize,1)\n",
        " print(input_shape)"
      ],
      "metadata": {
        "id": "iS4vUC1hxbGP"
      },
      "id": "iS4vUC1hxbGP",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential() #initializing sequential model, layers can be added sequentially with model.add\n",
        "model.add(Input(input_shape))\n",
        "model.add(SimpleRNN(12)) #simple recurrent layer, 10 neurons & process 10x1 sequences\n",
        "model.add(Dense(1,activation='linear')) #linear output because this is a regression problem\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "2YN-6VfexcqD"
      },
      "id": "2YN-6VfexcqD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 12 units\n",
        "- recurrent weights (states/activations are fed recursively): matrix $\\mathbf{W_{aa}}(u,u)$\n",
        "- input weights: n. units x n. features (here the number of features is 1: one sequence of previous values). For the blood pressure example, this would be 2 (diastolic and sistolic pressure values)\n",
        "- one bias term per unit\n",
        "\n",
        "$$\n",
        "168 = 12 \\text{ units} \\cdot 12 \\text{ units} + 12 \\text{ units} \\cdot 1 \\text{ feature} + 12 \\text{ bias terms}\n",
        "$$"
      ],
      "metadata": {
        "id": "BWkiL28GxgsW"
      },
      "id": "BWkiL28GxgsW"
    },
    {
      "cell_type": "code",
      "source": [
        "## let's fit the model\n",
        "loss_function = 'mse'\n",
        "optimizer_algorithm = 'Adam'\n",
        "metrics_list = ['mae',tf.keras.metrics.RootMeanSquaredError()]\n",
        "n_epochs = 20\n",
        "\n",
        "model.compile(loss=loss_function, optimizer=optimizer_algorithm, metrics=metrics_list)\n",
        "history = model.fit(X_train,y_train,epochs=n_epochs, verbose=1, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "KCYKqOfsxeo5"
      },
      "id": "KCYKqOfsxeo5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(history.history.keys())\n",
        "plt.plot(history.history['mae'])"
      ],
      "metadata": {
        "id": "YcouwurNxjKJ"
      },
      "id": "YcouwurNxjKJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## let's look at predictions\n",
        "predictions = model.predict(X_test)\n",
        "nrow = len(y_test)\n",
        "temp = y_test.reshape(nrow,1)\n",
        "\n",
        "## MAPE\n",
        "ptc_err = 100*(abs(predictions - temp)/abs(temp))\n",
        "mape = ptc_err.mean() # mean absolute percentage error\n",
        "\n",
        "## RMSE\n",
        "sqerr = (predictions - y_test)**2\n",
        "rmse = math.sqrt(sqerr.mean())\n",
        "\n",
        "## CORRELATION\n",
        "\n",
        "\n",
        "print(\"accuracy (measured as MAPE) is: \", round(mape,2), \" %\")\n",
        "print(\"accuracy (measured as RMSE) is: \", round(rmse,2), \" degrees\")"
      ],
      "metadata": {
        "id": "bPxLWzulxm7x"
      },
      "id": "bPxLWzulxm7x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pd.Series(predictions[:,0])\n",
        "y = pd.Series(y_test.astype(float))\n",
        "\n",
        "## CORRELATION\n",
        "print(y_pred.corr(y))"
      ],
      "metadata": {
        "id": "ixb56tkaxonB"
      },
      "id": "ixb56tkaxonB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_pct = 100*round(rmse/y_test.mean(),4)\n",
        "print(\"RMSE as % of the mean is:\", rmse_pct, \"%\")"
      ],
      "metadata": {
        "id": "3R4yhAAurbtV"
      },
      "id": "3R4yhAAurbtV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The accuracy of our **first simple RNN model** is:\n",
        "\n",
        "- RMSE = **6.71** degrees (33.26% of the mean)\n",
        "- Pearson correlation = **0.643**"
      ],
      "metadata": {
        "id": "6o9vIGl6phC1"
      },
      "id": "6o9vIGl6phC1"
    },
    {
      "cell_type": "code",
      "source": [
        "res = pd.DataFrame(dict(y_test = y, y_pred = y_pred)).reset_index()\n",
        "print(res)\n",
        "res[['y_test','y_pred']].plot()"
      ],
      "metadata": {
        "id": "IUU9R5texqpA"
      },
      "id": "IUU9R5texqpA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why is it so? We see that predictions fail to capture the highest temperatures at both extremes (left and right, australian summer).\n",
        "\n",
        "**QUESTION: what is going on?**"
      ],
      "metadata": {
        "id": "jYykEbJpxuko"
      },
      "id": "jYykEbJpxuko"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data preprocessing\n",
        "\n",
        "We may want to consider to normalise the data, that now are on the celsius degree scale.\n",
        "\n",
        "First, we calculate the average and the standard deviation on the training data:"
      ],
      "metadata": {
        "id": "5kkNTeZxxxnr"
      },
      "id": "5kkNTeZxxxnr"
    },
    {
      "cell_type": "code",
      "source": [
        "avg = df['Temperature'][:n].mean()\n",
        "dev_std = df['Temperature'][:n].std()"
      ],
      "metadata": {
        "id": "29pHI3dzxsKY"
      },
      "id": "29pHI3dzxsKY",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we can normalize (standardize) the data (both training and test):"
      ],
      "metadata": {
        "id": "d_eMgR4Ox1oh"
      },
      "id": "d_eMgR4Ox1oh"
    },
    {
      "cell_type": "code",
      "source": [
        "train = (train-avg)/dev_std\n",
        "test = (test-avg)/dev_std"
      ],
      "metadata": {
        "id": "jKldTBxuxzg6"
      },
      "id": "jKldTBxuxzg6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train"
      ],
      "metadata": {
        "id": "zfhjxbH3x5uJ"
      },
      "id": "zfhjxbH3x5uJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(train)"
      ],
      "metadata": {
        "id": "HSV6Nh62x6BS"
      },
      "id": "HSV6Nh62x6BS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(test)"
      ],
      "metadata": {
        "id": "23xJ2Adax7uH"
      },
      "id": "23xJ2Adax7uH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we need to recreate the sliding-window representation (again, both for the training and the test data):"
      ],
      "metadata": {
        "id": "Wop5scIIx_2_"
      },
      "id": "Wop5scIIx_2_"
    },
    {
      "cell_type": "code",
      "source": [
        "## prepare the training data\n",
        "windowSize, X_train, y_train = 10, [], [] #initialize lists and set window size\n",
        "for index in range(len(train)-windowSize): #we must end at train-windowSize to avoid the windowSize going past the end\n",
        "    X_train.append(train[index:index+windowSize]) #append the range from index to index+windowSize to x\n",
        "    y_train.append(train[index+windowSize]) #append the next value to the y\n",
        "\n",
        "X_train,y_train = np.array(X_train), np.array(y_train) #convert to numpy array\n",
        "X_train = X_train.reshape((len(X_train), windowSize, 1)) #reshape X_train to proper 3-d array\n",
        "\n",
        "print(y_train.shape)\n",
        "print(X_train.shape)"
      ],
      "metadata": {
        "id": "iFWzljXEx_hs"
      },
      "id": "iFWzljXEx_hs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## prepare the test data\n",
        "windowSize, X_test, y_test = 10, [], [] #initialize lists and set window size\n",
        "for index in range(len(test)-windowSize): #we must end at train-windowSize to avoid the windowSize going past the end\n",
        "    X_test.append(test[index:index+windowSize]) #append the range from index to index+windowSize to x\n",
        "    y_test.append(test[index+windowSize]) #append the next value to the y\n",
        "\n",
        "X_test,y_test = np.array(X_test), np.array(y_test) #convert to numpy array\n",
        "X_test = X_test.reshape((len(X_test), windowSize, 1)) #reshape X_train to proper 3-d array\n",
        "\n",
        "print(y_test.shape)\n",
        "print(X_test.shape)"
      ],
      "metadata": {
        "id": "t_IHdoW4yBtZ"
      },
      "id": "t_IHdoW4yBtZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train[0,0:10,0]"
      ],
      "metadata": {
        "id": "aG68KXfe5YrL"
      },
      "id": "aG68KXfe5YrL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train[0]"
      ],
      "metadata": {
        "id": "rl1T4Tzm5btc"
      },
      "id": "rl1T4Tzm5btc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's fit again the RNN model:"
      ],
      "metadata": {
        "id": "jXBqUTRHyExR"
      },
      "id": "jXBqUTRHyExR"
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Input(input_shape)) #initializing sequential model, layers can be added sequentially with model.add\n",
        "model.add(SimpleRNN(12)) #simple recurrent layer, 10 neurons & process 10x1 sequences\n",
        "model.add(Dense(1,activation='linear')) #linear output because this is a regression problem\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "nApOCzsDyDDr"
      },
      "id": "nApOCzsDyDDr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## let's fit the model\n",
        "loss_function = 'mse'\n",
        "optimizer_algorithm = 'Adam'\n",
        "metrics_list = ['mae',tf.keras.metrics.RootMeanSquaredError()]\n",
        "n_epochs = 20\n",
        "\n",
        "model.compile(loss=loss_function, optimizer=optimizer_algorithm, metrics=metrics_list)\n",
        "history = model.fit(X_train,y_train,epochs=n_epochs, verbose=1, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "5XpvikeLyGhS"
      },
      "id": "5XpvikeLyGhS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['mae'])"
      ],
      "metadata": {
        "id": "vlz98TXyyH7S"
      },
      "id": "vlz98TXyyH7S",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now, the predictions: first, on the standardized scale"
      ],
      "metadata": {
        "id": "em0EJ51vyNUS"
      },
      "id": "em0EJ51vyNUS"
    },
    {
      "cell_type": "code",
      "source": [
        "## let's look at predictions\n",
        "predictions = model.predict(X_test)\n",
        "nrow = len(y_test)\n",
        "temp = y_test.reshape(nrow,1)\n",
        "\n",
        "## MAPE\n",
        "ptc_err = 100*(abs(predictions - temp)/abs(temp))\n",
        "mape = ptc_err.mean() # mean absolute percentage error\n",
        "\n",
        "## RMSE\n",
        "sqerr = (predictions - y_test)**2\n",
        "rmse = math.sqrt(sqerr.mean())\n",
        "\n",
        "print(\"accuracy (measured as MAPE) is: \", round(mape,2), \" %\")\n",
        "print(\"accuracy (measured as RMSE) is: \", round(rmse,2), \" dev std\")"
      ],
      "metadata": {
        "id": "pQQTIwrWyLLB"
      },
      "id": "pQQTIwrWyLLB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pd.Series(predictions[:,0])\n",
        "y = pd.Series(y_test.astype(float))\n",
        "\n",
        "## CORRELATION\n",
        "print(y_pred.corr(y))"
      ],
      "metadata": {
        "id": "PE8VLJTwyPBL"
      },
      "id": "PE8VLJTwyPBL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that the shape of the test data is now better captured:"
      ],
      "metadata": {
        "id": "FZxoo26VyS4o"
      },
      "id": "FZxoo26VyS4o"
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(dict(y_test = y, y_pred = y_pred)).reset_index()\n",
        "print(df)\n",
        "df[['y_test','y_pred']].plot()"
      ],
      "metadata": {
        "id": "pc-E8bnuyVf5"
      },
      "id": "pc-E8bnuyVf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Backtransform the data\n",
        "\n",
        "Let's put our data back on the Celsius degrees scale:\n",
        "\n",
        "$$\n",
        "x_{degrees} = \\sigma \\cdot x_{std} + μ\n",
        "$$"
      ],
      "metadata": {
        "id": "r5iKjBidJTvh"
      },
      "id": "r5iKjBidJTvh"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred = pd.Series(predictions[:,0])\n",
        "y_pred = (y_pred * dev_std) + avg\n",
        "y = pd.Series(y_test.astype(float))\n",
        "y = (y * dev_std) + avg\n",
        "\n",
        "df = pd.DataFrame(dict(y_test = y, y_pred = y_pred)).reset_index()\n",
        "print(df)\n",
        "df[['y_test','y_pred']].plot()"
      ],
      "metadata": {
        "id": "wu8b1nTOyXCx"
      },
      "id": "wu8b1nTOyXCx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MAPE\n",
        "ptc_err = 100*(abs(y_pred - y)/abs(y))\n",
        "mape = ptc_err.mean() # mean absolute percentage error\n",
        "\n",
        "## RMSE\n",
        "sqerr = (y_pred - y)**2\n",
        "rmse = math.sqrt(sqerr.mean())\n",
        "\n",
        "print(\"accuracy (measured as MAPE) is: \", round(mape,2), \" %\")\n",
        "print(\"accuracy (measured as RMSE) is: \", round(rmse,2), \" degrees\")"
      ],
      "metadata": {
        "id": "Gj9K94YpJXTF"
      },
      "id": "Gj9K94YpJXTF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CORRELATION\n",
        "print(y_pred.corr(y))"
      ],
      "metadata": {
        "id": "4yd6HOd_96wJ"
      },
      "id": "4yd6HOd_96wJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rmse_pct = 100*round(rmse/y.mean(),4)\n",
        "print(\"RMSE as % of the mean is:\", rmse_pct, \"%\")"
      ],
      "metadata": {
        "id": "0JIFZbDa_MNf"
      },
      "id": "0JIFZbDa_MNf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "RMSE = 6.71 degrees (33.26% of the mean)\n",
        "Pearson correlation = 0.643\n",
        "\n",
        "|      | simple RNN | + norm  | norm. backtr |\n",
        "| ---- | -------------| ----- | ------------ |\n",
        "| RMSE | 6.71 °C | 1.24 std dev | 3.72 °C |\n",
        "| RMSE pct | 33.26% | NA | 18.41 % |\n",
        "| Pearson correlation | 0.643 | 0.783 | 0.783 |\n",
        "\n",
        "\n",
        "We see that normalization clearly improves the model performance:\n",
        "\n",
        "- RMSE is almost halved\n",
        "- Pearson correlation is up from 0.643 to 0.783 (0.14 points)\n",
        "\n",
        "Notice that:\n",
        "- RMSE as % of the mean is not applicable with normalised (standardised) data $\\rightarrow$ data are centered! (mean = 0)\n",
        "- Pearson correlation does not change with backtransformation (scale invariant)"
      ],
      "metadata": {
        "id": "syc7V5pa58P2"
      },
      "id": "syc7V5pa58P2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LSTM and GRU models\n",
        "\n",
        "Let's now change the architecture of our RNN model, by using a specialized type of unit: the **long-short term memory** (LSTM) unit (see [here](https://keras.io/api/layers/recurrent_layers/lstm/)) (below, we also try a GRU model).\n",
        "\n",
        "We use the same general architecture:\n",
        "- one hidden layer with 12 units"
      ],
      "metadata": {
        "id": "jLW2pteSoO01"
      },
      "id": "jLW2pteSoO01"
    },
    {
      "cell_type": "code",
      "source": [
        "print(y_train[0])\n",
        "print(X_train[0,0:10,0])\n",
        "print(X_test[0,0:10,0])\n",
        "print(y_test[0])"
      ],
      "metadata": {
        "id": "8cqbmc6foPac"
      },
      "id": "8cqbmc6foPac",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import LSTM\n",
        "\n",
        "model = Sequential() #initialize sequential model\n",
        "model.add(Input(input_shape))\n",
        "model.add(LSTM(12, return_sequences=False)) #LSTM layer with 12 units; return_sequences=True must be used only between RNN layers (not before a Dense layer)\n",
        "model.add(Dense(1,activation='linear')) #Dense output layer with 1 unit, linear activation\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ABB0TrsvAZe0"
      },
      "id": "ABB0TrsvAZe0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that we have more parameters than in a simple RNN model:\n",
        "- **685 vs 181**"
      ],
      "metadata": {
        "id": "ehM6XStGFC4B"
      },
      "id": "ehM6XStGFC4B"
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = 'mse'\n",
        "optimizer_algorithm = 'Adam'\n",
        "metrics_list = ['mae',tf.keras.metrics.RootMeanSquaredError()]\n",
        "n_epochs = 20\n",
        "\n",
        "model.compile(loss=loss_function, optimizer=optimizer_algorithm, metrics=metrics_list)\n",
        "history = model.fit(X_train,y_train,epochs=n_epochs, verbose=1, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "TKttbf5SAgRc"
      },
      "id": "TKttbf5SAgRc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['mae'])"
      ],
      "metadata": {
        "id": "tRee88uhAlPc"
      },
      "id": "tRee88uhAlPc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)\n",
        "y_pred = pd.Series(predictions[:,0])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "K2YGS1iZCunX"
      },
      "id": "K2YGS1iZCunX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "y_pred = pd.Series(predictions[:,0])\n",
        "y_pred = (y_pred * dev_std) + avg\n",
        "y = pd.Series(y_test.astype(float))\n",
        "y = (y * dev_std) + avg\n",
        "\n",
        "df = pd.DataFrame(dict(y_test = y, y_pred = y_pred)).reset_index()\n",
        "print(df)\n",
        "df[['y_test','y_pred']].plot()"
      ],
      "metadata": {
        "id": "0h3O0NfcAn6g"
      },
      "id": "0h3O0NfcAn6g",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MAPE\n",
        "ptc_err = 100*(abs(y_pred - y)/abs(y))\n",
        "mape = ptc_err.mean() # mean absolute percentage error\n",
        "\n",
        "## RMSE\n",
        "sqerr = (y_pred - y)**2\n",
        "rmse = math.sqrt(sqerr.mean())\n",
        "\n",
        "print(\"accuracy (measured as MAPE) is: \", round(mape,2), \" %\")\n",
        "print(\"accuracy (measured as RMSE) is: \", round(rmse,2), \" degrees\")"
      ],
      "metadata": {
        "id": "LJCmBoIKA2C7"
      },
      "id": "LJCmBoIKA2C7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CORRELATION\n",
        "print(y_pred.corr(y))"
      ],
      "metadata": {
        "id": "srPKzsv3A4rg"
      },
      "id": "srPKzsv3A4rg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRU\n",
        "\n",
        "We try now the GRU layer (same overall architecture)"
      ],
      "metadata": {
        "id": "54Bui_ifBPtD"
      },
      "id": "54Bui_ifBPtD"
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.layers import GRU\n",
        "\n",
        "model = Sequential() #initialize sequential model\n",
        "model.add(Input(input_shape))\n",
        "#model.add(GRU(12, return_sequences=True)) #GRU layer with 12 nodes; return_sequences=True must be used only between RNN layers (not before a Dense layer)\n",
        "model.add(GRU(12)) #GRU layer with 10 neurons\n",
        "model.add(Dense(1,activation='linear')) #Dense output layer with 1 neuron, linear activation\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "m1UrmfJRBQgk"
      },
      "id": "m1UrmfJRBQgk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also in this case, we have more parameters than with a simple RNN model (but fewer than the LSTM model):\n",
        "- **553 vs 181**"
      ],
      "metadata": {
        "id": "fH3Y-fs8FYAR"
      },
      "id": "fH3Y-fs8FYAR"
    },
    {
      "cell_type": "code",
      "source": [
        "loss_function = 'mse'\n",
        "optimizer_algorithm = 'Adam'\n",
        "metrics_list = ['mae',tf.keras.metrics.RootMeanSquaredError()]\n",
        "n_epochs = 20\n",
        "\n",
        "model.compile(loss=loss_function, optimizer=optimizer_algorithm, metrics=metrics_list)\n",
        "history = model.fit(X_train,y_train,epochs=n_epochs, verbose=1, validation_data=(X_test, y_test))"
      ],
      "metadata": {
        "id": "T3j0t88oBcuo"
      },
      "id": "T3j0t88oBcuo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(history.history['mae'])"
      ],
      "metadata": {
        "id": "LJbmjG0dBk--"
      },
      "id": "LJbmjG0dBk--",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(X_test)\n",
        "\n",
        "y_pred = pd.Series(predictions[:,0])\n",
        "y_pred = (y_pred * dev_std) + avg\n",
        "y = pd.Series(y_test.astype(float))\n",
        "y = (y * dev_std) + avg\n",
        "\n",
        "df = pd.DataFrame(dict(y_test = y, y_pred = y_pred)).reset_index()\n",
        "print(df)\n",
        "df[['y_test','y_pred']].plot()"
      ],
      "metadata": {
        "id": "6_aUPOHLBpSs"
      },
      "id": "6_aUPOHLBpSs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## MAPE\n",
        "ptc_err = 100*(abs(y_pred - y)/abs(y))\n",
        "mape = ptc_err.mean() # mean absolute percentage error\n",
        "\n",
        "## RMSE\n",
        "sqerr = (y_pred - y)**2\n",
        "rmse = math.sqrt(sqerr.mean())\n",
        "\n",
        "print(\"accuracy (measured as MAPE) is: \", round(mape,2), \" %\")\n",
        "print(\"accuracy (measured as RMSE) is: \", round(rmse,2), \" degrees\")"
      ],
      "metadata": {
        "id": "C_edxIWqBtvr"
      },
      "id": "C_edxIWqBtvr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## CORRELATION\n",
        "print(y_pred.corr(y))"
      ],
      "metadata": {
        "id": "S24L2molBxoA"
      },
      "id": "S24L2molBxoA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With both LSTM and GRU layers, we get the same results as with the SimpleRNN model:\n",
        "\n",
        "- sequences are not very long (10 days temperature)\n",
        "- the SimpleRNN model already seems to have reached a plateau in terms of loss and accuracy"
      ],
      "metadata": {
        "id": "r3B2g0f6FibA"
      },
      "id": "r3B2g0f6FibA"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}