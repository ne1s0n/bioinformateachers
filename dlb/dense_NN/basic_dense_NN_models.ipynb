{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "16f2f29e",
      "metadata": {
        "id": "16f2f29e"
      },
      "source": [
        "# Basic dense (fully connected) neural network models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic simple neural network model\n",
        "\n",
        "In the previous notebooks, we saw how to implement simple neural network models with **just the output layer** for logistic, softmax (multiclass) and linear regression problems: this output layer had **only one node** (logistic and linear regression) which performed both the linear combination of input variables + bias and the sigmoid/linear activation:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1PRc719uT1kOUuCMbpHML2sEk7qp6UJnm\">\n",
        "\n",
        "(Softmax regression is slightly different: the single output layer has as many nodes as there are classes, each calculating the linear combination of input variables and the softmax activation).\n"
      ],
      "metadata": {
        "id": "cgmleXAaWYWB"
      },
      "id": "cgmleXAaWYWB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic dense neural network model\n",
        "\n",
        "We are now building a **neural network model**, by adding **one hidden layer** (not deep) with **u nodes** (units):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QROz9pFnMoqTeqrFbele8pFz8qXDSckq\">\n",
        "\n",
        "There's a number of `hyperparameters`:\n",
        "\n",
        "- the **number of hidden nodes** (number of units in the hidden layer)\n",
        "- the **type of activation function** in the hidden layer\n",
        "- the **output activation function**\n",
        "- the **loss function** (for backpropagation)\n",
        "- the **optimizer** (for gradient descent)\n",
        "\n",
        "By stacking together more than one hidden/intermediate layer (additional hyperparameter), we can then build **deep neural networks**."
      ],
      "metadata": {
        "id": "xQl8moH7c_q2"
      },
      "id": "xQl8moH7c_q2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries and setting the random seed\n",
        "\n",
        "First of all, we load some necessary libraries; then we setup the random seed to ensure reproducibility of results. Since tensorflow uses an internal random generator we need to fix both the general seed (via numpy `seed()`) and tensorflow seed (via `set_seet()`)"
      ],
      "metadata": {
        "id": "5Z63YlH0OgfH"
      },
      "id": "5Z63YlH0OgfH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e86c1bf",
      "metadata": {
        "id": "9e86c1bf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "  # Set the seed using keras.utils.set_random_seed. This will set:\n",
        "  # 1) `numpy` seed\n",
        "  # 2) `tensorflow` random seed\n",
        "  # 3) `python` random seed\n",
        "tf.keras.utils.set_random_seed(15)\n",
        "\n",
        "  # This will make TensorFlow ops as deterministic as possible, but it will\n",
        "  # affect the overall performance, so it's not enabled by default.\n",
        "  # `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data\n",
        "\n",
        "We get the usual `iris` dataset:"
      ],
      "metadata": {
        "id": "zZ3vzxq4B0IQ"
      },
      "id": "zZ3vzxq4B0IQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09318d92",
      "metadata": {
        "id": "09318d92"
      },
      "outputs": [],
      "source": [
        "import sklearn.datasets\n",
        "\n",
        "(features, target) = sklearn.datasets.load_iris(return_X_y = True) ## feature names are not returned\n",
        "print(features.shape)\n",
        "print(target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a three-class problem, and for the logistic regression example we need to binarise it:"
      ],
      "metadata": {
        "id": "wBxq3lGLu00F"
      },
      "id": "wBxq3lGLu00F"
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(target, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "metadata": {
        "id": "n0I1rUV9uWdI"
      },
      "id": "n0I1rUV9uWdI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#updating class labels. To makes things difficult we put together old classes 0 and 1\n",
        "#in a new class (non virginica) and keep old class 2 (virginica) as new class 1.\n",
        "#For an easier problems put together versicolor and virginica and keep setosa by itself\n",
        "j = 100 ## split: 50 for setosa vs versicolor+virginica, 100 for setosa+versicolor vs virginica\n",
        "binary_target = np.copy(target)\n",
        "binary_target[0:j] = 0\n",
        "binary_target[j:150] = 1"
      ],
      "metadata": {
        "id": "tTyIWlhLuTB4"
      },
      "id": "tTyIWlhLuTB4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(binary_target, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "metadata": {
        "id": "qZNIOp89vbMQ"
      },
      "id": "qZNIOp89vbMQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and validation sets"
      ],
      "metadata": {
        "id": "Nz8nRg1_HyjY"
      },
      "id": "Nz8nRg1_HyjY"
    },
    {
      "cell_type": "code",
      "source": [
        "#we want to have the same proportion of classes in both train and validation sets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n",
        "#assigned to validation set (here called \"test\")\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "\n",
        "#the .split() method returns (an iterable over) two lists which can be\n",
        "#used to index the samples that go into train and validation sets\n",
        "for train_index, val_index in sss.split(features, binary_target):\n",
        "    features_train = features[train_index, :]\n",
        "    features_val   = features[val_index, :]\n",
        "    target_train   = binary_target[train_index]\n",
        "    target_val     = binary_target[val_index]\n",
        "\n",
        "#let's print some shapes to get an idea of the resulting data structure\n",
        "print(features_train.shape)\n",
        "print(features_val.shape)\n",
        "print(target_train.shape)\n",
        "print(target_val.shape)"
      ],
      "metadata": {
        "id": "kCZCcaW3uBgg"
      },
      "id": "kCZCcaW3uBgg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(Counter(target_train))\n",
        "print(Counter(target_val))"
      ],
      "metadata": {
        "id": "HVoTq9-twsIF"
      },
      "id": "HVoTq9-twsIF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_train"
      ],
      "metadata": {
        "id": "5A6QNY5k0AYs"
      },
      "id": "5A6QNY5k0AYs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the neural network model\n",
        "\n",
        "We now build our neural network for binary classification: it will be comprised of one intermediate layer and one output layer which that will perform the final classification (actually, the calculation of the probability of belonging to class `1` given the input features: $P(y=1|x$)).\n",
        "\n",
        "The necessary steps are:\n",
        "\n",
        "- model set-up (define the hyperparameters)\n",
        "- model architecture\n",
        "- compiling (putting together the configuration -model set-up- and the architecture)"
      ],
      "metadata": {
        "id": "vjefipM6w5rq"
      },
      "id": "vjefipM6w5rq"
    },
    {
      "cell_type": "code",
      "source": [
        "## # Configuration options\n",
        "input_shape = (features.shape[1],) ## tuple that specifies the number of features\n",
        "hidden_nodes = 8\n",
        "hidden_activation = 'relu'\n",
        "output_activation = 'sigmoid'\n",
        "loss_function = 'binary_crossentropy'\n",
        "optimizer_used = 'SGD' ##stochastic gradient descent\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "7bfjgppxe8Oe"
      },
      "id": "7bfjgppxe8Oe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we are building a \"sequential\" model, meaning that the data will\n",
        "#flow like INPUT -> ELABORATION -> OUTPUT. In particular, we will\n",
        "#not have any loops, i.e. our output will never be recycled as\n",
        "#input for the first layer\n",
        "from keras.models import Sequential\n",
        "\n",
        "#a \"dense\" layer is a layer were all the data coming in are connected\n",
        "#to all nodes (fully connected).\n",
        "from keras.layers import Dense, Input\n",
        "\n",
        "# 2-class logistic regression in Keras\n",
        "model = Sequential()\n",
        "model.add(Input(input_shape))\n",
        "model.add(Dense(units=hidden_nodes, activation=hidden_activation))\n",
        "model.add(Dense(units=1, activation=output_activation))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function)"
      ],
      "metadata": {
        "id": "7-OPng9vwsfr"
      },
      "id": "7-OPng9vwsfr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "U-R-j2XCyEdh"
      },
      "id": "U-R-j2XCyEdh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `summary()` method of the Keras model tells us that there are 49  parameters to train:\n",
        "- w1, w2, w3, w4, b (weights for the 4 features + bias term) for each of the 8 nodes in the hidden layer ($\\rightarrow$ (4+1) x 8 = 40 parameters);\n",
        "- w1 - w8 + b: weights for the results from the 8 intermediate nodes (\"new features\") + bias term, for the output layer ($\\rightarrow$ 8 + 1 = 9 parameters)\n",
        "- layer 1 (40 parameters) + layer 2 (9 parameters) = 49 total parameters"
      ],
      "metadata": {
        "id": "xwCbMT88yi9T"
      },
      "id": "xwCbMT88yi9T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the neural network"
      ],
      "metadata": {
        "id": "5sBeDq2Jqobd"
      },
      "id": "5sBeDq2Jqobd"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_val, target_val), verbose=0)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "q1VSjPS_yrGO",
        "collapsed": true
      },
      "id": "q1VSjPS_yrGO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to take a look at losses evolution\n",
        "def plot_loss_history(h, title):\n",
        "    plt.plot(h.history['loss'], label = \"Train loss\")\n",
        "    plt.plot(h.history['val_loss'], label = \"Validation loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "T1suZSVVzhSO"
      },
      "id": "T1suZSVVzhSO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_history(history, 'Logistic (' + str(num_epochs) + ' epochs)')"
      ],
      "metadata": {
        "id": "o0wcznnUzjHd"
      },
      "id": "o0wcznnUzjHd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model evaluation\n",
        "\n",
        "Any model is only useful when it's used to predict new, unknown data. The validation set was put apart and not really used for training for this specific reason.\n",
        "\n",
        "Here we look at the following ways to evaluate our neural network model:\n",
        "\n",
        "- error-rate / accuracy\n",
        "- confusion matrix\n",
        "\n",
        "To calculate the accuracy of the trained neural network model for binary classification, we first need to get the **predictions made on the validation set**.\n",
        "Luckily, it's very easy to apply a trained model to new data (the validation set) via the [predict() method](https://keras.io/api/models/model_training_apis/#predict-method).\n",
        "\n",
        "We can thus get our prediction for the iris flowers (see below the first 5 predictions):"
      ],
      "metadata": {
        "id": "ctvEtwttqu4G"
      },
      "id": "ctvEtwttqu4G"
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(features_val)\n",
        "print(predictions[0:5])"
      ],
      "metadata": {
        "id": "LV558dWEqa8I"
      },
      "id": "LV558dWEqa8I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot the histogram of predictions (in the interval [0,1]), alongside the **0.5 classification threshold**"
      ],
      "metadata": {
        "id": "ekyTG-p7yL3m"
      },
      "id": "ekyTG-p7yL3m"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(predictions)\n",
        "plt.axvline(0.5, color='red', linestyle='dashed', linewidth=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FzzmLMx8q8PX"
      },
      "id": "FzzmLMx8q8PX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Error rate / accuracy"
      ],
      "metadata": {
        "id": "Oed7HBh6ysnw"
      },
      "id": "Oed7HBh6ysnw"
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class = np.where(predictions > 0.5, \"virginica\", \"non-virginica\")\n",
        "target_class = np.where(target_val == 1, \"virginica\", \"non-virginica\")\n",
        "target_class = target_class.reshape(len(target_class),1)\n",
        "\n",
        "results = target_class == predicted_class"
      ],
      "metadata": {
        "id": "WVTmht74rD22"
      },
      "id": "WVTmht74rD22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors = np.invert(results).sum()\n",
        "correct_predictions = results.sum()\n",
        "total_n_predictions = len(results)\n",
        "\n",
        "print(\"Error rate:\", round(errors/total_n_predictions, 3))\n",
        "print(\"Accuracy:\", round(correct_predictions/total_n_predictions, 3))"
      ],
      "metadata": {
        "id": "lZUcLftuynLj"
      },
      "id": "lZUcLftuynLj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Confusion matrix"
      ],
      "metadata": {
        "id": "-bAXc6o7zBod"
      },
      "id": "-bAXc6o7zBod"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "labels = ['non-virginica','virginica']\n",
        "con_mat_df = confusion_matrix( y_true = target_class, y_pred = predicted_class, labels=labels) #true are rows, predicted are columns\n",
        "pd.DataFrame(\n",
        "    con_mat_df,\n",
        "    index = ['true:'+x for x in labels],\n",
        "    columns = ['pred:'+x for x in labels])"
      ],
      "metadata": {
        "id": "EqlVZ3i1yxbg"
      },
      "id": "EqlVZ3i1yxbg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What if we want to add more layers?\n",
        "\n",
        "This is very simple: you just need to specify one (or more) additional layer(s): see the example below.\n",
        "\n",
        "For any additional layer, you need also to specify the number of units and the activation function (additional hyperparameters to fine-tune: the number of layers is itself another hyperparameter to tune)."
      ],
      "metadata": {
        "id": "TjYr-9p9zGfc"
      },
      "id": "TjYr-9p9zGfc"
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (features_train.shape[1],) ## tuple that specifies the number of features\n",
        "hidden_nodes_1 = 8\n",
        "hidden_nodes_2 = 5\n",
        "hidden_activation_1 = 'relu'\n",
        "hidden_activation_2 = 'relu'\n",
        "output_activation = 'sigmoid'\n",
        "loss_function = 'binary_crossentropy'\n",
        "optimizer_used = 'rmsprop' ## Root Mean Square Propagation\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "rdD49RZly0jB"
      },
      "id": "rdD49RZly0jB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## resetting the seed (new model graph by tensorflow: seed needs to be specified again)\n",
        "def reset_random_seeds(nseed, enable_determinism=False):\n",
        "    tf.keras.utils.set_random_seed(nseed)\n",
        "    #np.random.seed(n2)\n",
        "    if enable_determinism:\n",
        "        tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "reset_random_seeds(19)\n",
        "\n",
        "# binary classification shallow neural network model in Keras\n",
        "model = Sequential()\n",
        "model.add(tf.keras.Input(input_shape))\n",
        "model.add(Dense(units=hidden_nodes_1, activation=hidden_activation_1))\n",
        "model.add(Dense(units=hidden_nodes_2, activation=hidden_activation_2))\n",
        "model.add(Dense(1, activation=output_activation))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function)"
      ],
      "metadata": {
        "id": "MC4lZLjA0Iiq"
      },
      "id": "MC4lZLjA0Iiq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "tcjELsG_0KL7"
      },
      "id": "tcjELsG_0KL7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>Parameters breakdown</u>:\n",
        "- layer 1: 8 x (4 + 1) = 40\n",
        "- layer 2: 5 x (8 + 1) = 45\n",
        "- layer 3: 1 x (5 + 1) = 6\n",
        "- 40 + 45 + 6 = 91 total parameters"
      ],
      "metadata": {
        "id": "BUk3bBwb1BiM"
      },
      "id": "BUk3bBwb1BiM"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_val, target_val), verbose=0)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "fAeUfO4206P1"
      },
      "id": "fAeUfO4206P1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It usually takes longer to train a larger neural network: not necessarily this translates into a better performance of the model."
      ],
      "metadata": {
        "id": "nUanzarO4xju"
      },
      "id": "nUanzarO4xju"
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(features_val)\n",
        "\n",
        "predicted_class = np.where(predictions > 0.5, \"virginica\", \"non-virginica\")\n",
        "target_class = np.where(target_val == 1, \"virginica\", \"non-virginica\")\n",
        "target_class = target_class.reshape(len(target_class),1)\n",
        "\n",
        "labels = ['non-virginica','virginica']\n",
        "con_mat_df = confusion_matrix( y_true = target_class, y_pred = predicted_class, labels=labels) #true are rows, predicted are columns\n",
        "pd.DataFrame(\n",
        "    con_mat_df,\n",
        "    index = ['true:'+x for x in labels],\n",
        "    columns = ['pred:'+x for x in labels])"
      ],
      "metadata": {
        "id": "NalQxfCt1aBl"
      },
      "id": "NalQxfCt1aBl",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}