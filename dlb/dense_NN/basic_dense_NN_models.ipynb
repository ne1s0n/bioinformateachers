{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "16f2f29e",
      "metadata": {
        "id": "16f2f29e"
      },
      "source": [
        "# Basic dense (fully connected) neural network models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic simple neural network model\n",
        "\n",
        "In the previous notebooks, we saw how to implement simple neural network models with **just the output layer** for logistic, softmax (multiclass) and linear regression problems: this output layer had **only one node** (logistic and linear regression) which performed both the linear combination of input variables + bias and the sigmoid/linear activation:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1PRc719uT1kOUuCMbpHML2sEk7qp6UJnm\">\n",
        "\n",
        "(Softmax regression is slightly different: the single output layer has as many nodes as there are classes, each calculating the linear combination of input variables and the softmax activation).\n"
      ],
      "metadata": {
        "id": "cgmleXAaWYWB"
      },
      "id": "cgmleXAaWYWB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic dense neural network model\n",
        "\n",
        "We are now building a **neural network model**, by adding **one hidden layer** (not deep) with **u nodes** (units):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QROz9pFnMoqTeqrFbele8pFz8qXDSckq\">\n",
        "\n",
        "There's a number of `hyperparameters`:\n",
        "\n",
        "- the **number of hidden nodes** (number of units in the hidden layer)\n",
        "- the **type of activation function** in the hidden layer\n",
        "- the **output activation function**\n",
        "- the **loss function** (for backpropagation)\n",
        "- the **optimizer** (for gradient descent)\n",
        "\n",
        "By stacking together more than one hidden/intermediate layer (additional hyperparameter), we can then build **deep neural networks**."
      ],
      "metadata": {
        "id": "xQl8moH7c_q2"
      },
      "id": "xQl8moH7c_q2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries and setting the random seed\n",
        "\n",
        "First of all, we load some necessary libraries; then we setup the random seed to ensure reproducibility of results. Since tensorflow uses an internal random generator we need to fix both the general seed (via numpy `seed()`) and tensorflow seed (via `set_seet()`)"
      ],
      "metadata": {
        "id": "5Z63YlH0OgfH"
      },
      "id": "5Z63YlH0OgfH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e86c1bf",
      "metadata": {
        "id": "9e86c1bf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "  # Set the seed using keras.utils.set_random_seed. This will set:\n",
        "  # 1) `numpy` seed\n",
        "  # 2) `tensorflow` random seed\n",
        "  # 3) `python` random seed\n",
        "tf.keras.utils.set_random_seed(15)\n",
        "\n",
        "  # This will make TensorFlow ops as deterministic as possible, but it will\n",
        "  # affect the overall performance, so it's not enabled by default.\n",
        "  # `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data\n",
        "\n",
        "We get the usual `iris` dataset:"
      ],
      "metadata": {
        "id": "zZ3vzxq4B0IQ"
      },
      "id": "zZ3vzxq4B0IQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09318d92",
      "metadata": {
        "id": "09318d92"
      },
      "outputs": [],
      "source": [
        "import sklearn.datasets\n",
        "\n",
        "(features, target) = sklearn.datasets.load_iris(return_X_y = True) ## feature names are not returned\n",
        "print(features.shape)\n",
        "print(target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a three-class problem, and for the logistic regression example we need to binarise it:"
      ],
      "metadata": {
        "id": "wBxq3lGLu00F"
      },
      "id": "wBxq3lGLu00F"
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(target, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "metadata": {
        "id": "n0I1rUV9uWdI"
      },
      "id": "n0I1rUV9uWdI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#updating class labels. To makes things difficult we put together old classes 0 and 1\n",
        "#in a new class (non virginica) and keep old class 2 (virginica) as new class 1.\n",
        "#For an easier problems put together versicolor and virginica and keep setosa by itself\n",
        "j = 100 ## split: 50 for setosa vs versicolor+virginica, 100 for setosa+versicolor vs virginica\n",
        "binary_target = np.copy(target)\n",
        "binary_target[0:j] = 0\n",
        "binary_target[j:150] = 1"
      ],
      "metadata": {
        "id": "tTyIWlhLuTB4"
      },
      "id": "tTyIWlhLuTB4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(binary_target, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "metadata": {
        "id": "qZNIOp89vbMQ"
      },
      "id": "qZNIOp89vbMQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and validation sets"
      ],
      "metadata": {
        "id": "Nz8nRg1_HyjY"
      },
      "id": "Nz8nRg1_HyjY"
    },
    {
      "cell_type": "code",
      "source": [
        "#we want to have the same proportion of classes in both train and validation sets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n",
        "#assigned to validation set (here called \"test\")\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "\n",
        "#the .split() method returns (an iterable over) two lists which can be\n",
        "#used to index the samples that go into train and validation sets\n",
        "for train_index, val_index in sss.split(features, binary_target):\n",
        "    features_train = features[train_index, :]\n",
        "    features_val   = features[val_index, :]\n",
        "    target_train   = binary_target[train_index]\n",
        "    target_val     = binary_target[val_index]\n",
        "\n",
        "#let's print some shapes to get an idea of the resulting data structure\n",
        "print(features_train.shape)\n",
        "print(features_val.shape)\n",
        "print(target_train.shape)\n",
        "print(target_val.shape)"
      ],
      "metadata": {
        "id": "kCZCcaW3uBgg"
      },
      "id": "kCZCcaW3uBgg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(Counter(target_train))\n",
        "print(Counter(target_val))"
      ],
      "metadata": {
        "id": "HVoTq9-twsIF"
      },
      "id": "HVoTq9-twsIF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_train"
      ],
      "metadata": {
        "id": "5A6QNY5k0AYs"
      },
      "id": "5A6QNY5k0AYs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the neural network model\n",
        "\n",
        "We now build our neural network for binary classification: it will be comprised of one intermediate layer and one output layer which that will perform the final classification (actually, the calculation of the probability of belonging to class `1` given the input features: $P(y=1|x$)).\n",
        "\n",
        "The necessary steps are:\n",
        "\n",
        "- model set-up (define the hyperparameters)\n",
        "- model architecture\n",
        "- compiling (putting together the configuration -model set-up- and the architecture)"
      ],
      "metadata": {
        "id": "vjefipM6w5rq"
      },
      "id": "vjefipM6w5rq"
    },
    {
      "cell_type": "code",
      "source": [
        "## # Configuration options\n",
        "input_shape = (features.shape[1],) ## tuple that specifies the number of features\n",
        "hidden_nodes = 8\n",
        "hidden_activation = 'relu'\n",
        "output_activation = 'sigmoid'\n",
        "loss_function = 'binary_crossentropy'\n",
        "optimizer_used = 'SGD' ##stochastic gradient descent\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "7bfjgppxe8Oe"
      },
      "id": "7bfjgppxe8Oe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we are building a \"sequential\" model, meaning that the data will\n",
        "#flow like INPUT -> ELABORATION -> OUTPUT. In particular, we will\n",
        "#not have any loops, i.e. our output will never be recycled as\n",
        "#input for the first layer\n",
        "from keras.models import Sequential\n",
        "\n",
        "#a \"dense\" layer is a layer were all the data coming in are connected\n",
        "#to all nodes (fully connected).\n",
        "from keras.layers import Dense, Input\n",
        "\n",
        "# 2-class logistic regression in Keras\n",
        "model = Sequential()\n",
        "model.add(Input(input_shape))\n",
        "model.add(Dense(units=hidden_nodes, activation=hidden_activation))\n",
        "model.add(Dense(units=1, activation=output_activation))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function)"
      ],
      "metadata": {
        "id": "7-OPng9vwsfr"
      },
      "id": "7-OPng9vwsfr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "U-R-j2XCyEdh"
      },
      "id": "U-R-j2XCyEdh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `summary()` method of the Keras model tells us that there are 49  parameters to train:\n",
        "- w1, w2, w3, w4, b (weights for the 4 features + bias term) for each of the 8 nodes in the hidden layer ($\\rightarrow$ (4+1) x 8 = 40 parameters);\n",
        "- w1 - w8 + b: weights for the results from the 8 intermediate nodes (\"new features\") + bias term, for the output layer ($\\rightarrow$ 8 + 1 = 9 parameters)\n",
        "- layer 1 (40 parameters) + layer 2 (9 parameters) = 49 total parameters"
      ],
      "metadata": {
        "id": "xwCbMT88yi9T"
      },
      "id": "xwCbMT88yi9T"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the neural network"
      ],
      "metadata": {
        "id": "5sBeDq2Jqobd"
      },
      "id": "5sBeDq2Jqobd"
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start = time.time()\n",
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_val, target_val), verbose=0)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "q1VSjPS_yrGO",
        "collapsed": true
      },
      "id": "q1VSjPS_yrGO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to take a look at losses evolution\n",
        "def plot_loss_history(h, title):\n",
        "    plt.plot(h.history['loss'], label = \"Train loss\")\n",
        "    plt.plot(h.history['val_loss'], label = \"Validation loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "T1suZSVVzhSO"
      },
      "id": "T1suZSVVzhSO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_history(history, 'Logistic (' + str(num_epochs) + ' epochs)')"
      ],
      "metadata": {
        "id": "o0wcznnUzjHd"
      },
      "id": "o0wcznnUzjHd",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model evaluation\n",
        "\n",
        "Any model is only useful when it's used to predict new, unknown data. The validation set was put apart and not really used for training for this specific reason.\n",
        "\n",
        "Here we look at the following ways to evaluate our neural network model:\n",
        "\n",
        "- error-rate / accuracy\n",
        "- confusion matrix\n",
        "\n",
        "To calculate the accuracy of the trained neural network model for binary classification, we first need to get the **predictions made on the validation set**.\n",
        "Luckily, it's very easy to apply a trained model to new data (the validation set) via the [predict() method](https://keras.io/api/models/model_training_apis/#predict-method).\n",
        "\n",
        "We can thus get our prediction for the iris flowers (see below the first 5 predictions):"
      ],
      "metadata": {
        "id": "ctvEtwttqu4G"
      },
      "id": "ctvEtwttqu4G"
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(features_val)\n",
        "print(predictions[0:5])"
      ],
      "metadata": {
        "id": "LV558dWEqa8I"
      },
      "id": "LV558dWEqa8I",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We plot the histogram of predictions (in the interval [0,1]), alongside the **0.5 classification threshold**"
      ],
      "metadata": {
        "id": "ekyTG-p7yL3m"
      },
      "id": "ekyTG-p7yL3m"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(predictions)\n",
        "plt.axvline(0.5, color='red', linestyle='dashed', linewidth=1)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "FzzmLMx8q8PX"
      },
      "id": "FzzmLMx8q8PX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Error rate / accuracy"
      ],
      "metadata": {
        "id": "Oed7HBh6ysnw"
      },
      "id": "Oed7HBh6ysnw"
    },
    {
      "cell_type": "code",
      "source": [
        "predicted_class = np.where(predictions > 0.5, \"virginica\", \"non-virginica\")\n",
        "target_class = np.where(target_val == 1, \"virginica\", \"non-virginica\")\n",
        "target_class = target_class.reshape(len(target_class),1)\n",
        "\n",
        "results = target_class == predicted_class"
      ],
      "metadata": {
        "id": "WVTmht74rD22"
      },
      "id": "WVTmht74rD22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "errors = np.invert(results).sum()\n",
        "correct_predictions = results.sum()\n",
        "total_n_predictions = len(results)\n",
        "\n",
        "print(\"Error rate:\", round(errors/total_n_predictions, 3))\n",
        "print(\"Accuracy:\", round(correct_predictions/total_n_predictions, 3))"
      ],
      "metadata": {
        "id": "lZUcLftuynLj"
      },
      "id": "lZUcLftuynLj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Confusion matrix"
      ],
      "metadata": {
        "id": "-bAXc6o7zBod"
      },
      "id": "-bAXc6o7zBod"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "labels = ['non-virginica','virginica']\n",
        "con_mat_df = confusion_matrix( y_true = target_class, y_pred = predicted_class, labels=labels) #true are rows, predicted are columns\n",
        "pd.DataFrame(\n",
        "    con_mat_df,\n",
        "    index = ['true:'+x for x in labels],\n",
        "    columns = ['pred:'+x for x in labels])"
      ],
      "metadata": {
        "id": "EqlVZ3i1yxbg"
      },
      "id": "EqlVZ3i1yxbg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What if we want to add more layers?\n",
        "\n",
        "This is very simple: you just need to specify one (or more) additional layer(s): see the example below.\n",
        "\n",
        "For any additional layer, you need also to specify the number of units and the activation function (additional hyperparameters to fine-tune: the number of layers is itself another hyperparameter to tune)."
      ],
      "metadata": {
        "id": "TjYr-9p9zGfc"
      },
      "id": "TjYr-9p9zGfc"
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = (features_train.shape[1],) ## tuple that specifies the number of features\n",
        "hidden_nodes_1 = 8\n",
        "hidden_nodes_2 = 5\n",
        "hidden_activation_1 = 'relu'\n",
        "hidden_activation_2 = 'relu'\n",
        "output_activation = 'sigmoid'\n",
        "loss_function = 'binary_crossentropy'\n",
        "optimizer_used = 'rmsprop' ## Root Mean Square Propagation\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "rdD49RZly0jB"
      },
      "id": "rdD49RZly0jB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## resetting the seed (new model graph by tensorflow: seed needs to be specified again)\n",
        "def reset_random_seeds(nseed, enable_determinism=False):\n",
        "    tf.keras.utils.set_random_seed(nseed)\n",
        "    #np.random.seed(n2)\n",
        "    if enable_determinism:\n",
        "        tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "reset_random_seeds(19)\n",
        "\n",
        "# binary classification shallow neural network model in Keras\n",
        "model = Sequential()\n",
        "model.add(tf.keras.Input(input_shape))\n",
        "model.add(Dense(units=hidden_nodes_1, activation=hidden_activation_1))\n",
        "model.add(Dense(units=hidden_nodes_2, activation=hidden_activation_2))\n",
        "model.add(Dense(1, activation=output_activation))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function)"
      ],
      "metadata": {
        "id": "MC4lZLjA0Iiq"
      },
      "id": "MC4lZLjA0Iiq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "tcjELsG_0KL7"
      },
      "id": "tcjELsG_0KL7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>Parameters breakdown</u>:\n",
        "- layer 1: 8 x (4 + 1) = 40\n",
        "- layer 2: 5 x (8 + 1) = 45\n",
        "- layer 3: 1 x (5 + 1) = 6\n",
        "- 40 + 45 + 6 = 91 total parameters"
      ],
      "metadata": {
        "id": "BUk3bBwb1BiM"
      },
      "id": "BUk3bBwb1BiM"
    },
    {
      "cell_type": "code",
      "source": [
        "start = time.time()\n",
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_val, target_val), verbose=0)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "fAeUfO4206P1"
      },
      "id": "fAeUfO4206P1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It usually takes longer to train a larger neural network: not necessarily this translates into a better performance of the model."
      ],
      "metadata": {
        "id": "nUanzarO4xju"
      },
      "id": "nUanzarO4xju"
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = model.predict(features_val)\n",
        "\n",
        "predicted_class = np.where(predictions > 0.5, \"virginica\", \"non-virginica\")\n",
        "target_class = np.where(target_val == 1, \"virginica\", \"non-virginica\")\n",
        "target_class = target_class.reshape(len(target_class),1)\n",
        "\n",
        "labels = ['non-virginica','virginica']\n",
        "con_mat_df = confusion_matrix( y_true = target_class, y_pred = predicted_class, labels=labels) #true are rows, predicted are columns\n",
        "pd.DataFrame(\n",
        "    con_mat_df,\n",
        "    index = ['true:'+x for x in labels],\n",
        "    columns = ['pred:'+x for x in labels])"
      ],
      "metadata": {
        "id": "NalQxfCt1aBl"
      },
      "id": "NalQxfCt1aBl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network model for multiclass classification\n",
        "\n",
        "We have here.\n",
        "\n",
        "- the **input layer** (conventionally known as `layer 0`: the data)\n",
        "- the **intermediate layer** (the core of the neural network)\n",
        "- the **output layer**: in this case, we have here as many units as there are classes in the problem (each unit calculates the probability of belonging to one class, i.e. a unique combination of the input data, with specific weights: all probabilities are then normalised by the softmax function)\n",
        "\n",
        "![multiclass_network](multiclass_classification_network.png)\n",
        "\n",
        "The output layer must use the softamx activation function; in the intermediate layer, you are free to choose whichever activation function you prefer (except the linear function which, as you should know by now, wouldn't work!).\n",
        "\n",
        "We use the original target vector (three classes):"
      ],
      "metadata": {
        "id": "QWQNYbS_9LiR"
      },
      "id": "QWQNYbS_9LiR"
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(target, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "metadata": {
        "id": "M-kM9OPA9Quk"
      },
      "id": "M-kM9OPA9Quk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#the \"utils\" subpackage is very useful, take a look to it when you have time\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "#converting to categorical\n",
        "target_multi_cat = tf.keras.utils.to_categorical(target)\n",
        "\n",
        "#let's take a look\n",
        "print(target_multi_cat[0:5,:])"
      ],
      "metadata": {
        "id": "aslKiE0-BxLp"
      },
      "id": "aslKiE0-BxLp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will now:\n",
        "\n",
        "1. split the data: training and validation sets\n",
        "2. data preprocessing\n",
        "3. set the hyperparameters: we now need to use the `softmax` activation function, and the `categorical_crossentropy` loss function\n",
        "4. build the model\n",
        "5. compile the model\n",
        "6. train the model\n",
        "7. evaluate the model"
      ],
      "metadata": {
        "id": "iGEhYdfX_fmh"
      },
      "id": "iGEhYdfX_fmh"
    },
    {
      "cell_type": "code",
      "source": [
        "## 1) SPLITTING THE DATA\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "\n",
        "#the .split() method returns (an iterable over) two lists which can be\n",
        "#used to index the samples that go into train and validation sets\n",
        "for train_index, val_index in sss.split(features, binary_target):\n",
        "    features_train = features[train_index, :]\n",
        "    features_val   = features[val_index, :]\n",
        "    target_train   = target_multi_cat[train_index,:]\n",
        "    target_val     = target_multi_cat[val_index,:]"
      ],
      "metadata": {
        "id": "hI71QrQz9mEU"
      },
      "id": "hI71QrQz9mEU",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#number of classes per split\n",
        "print('\\nClasses in train set:')\n",
        "print(target_train.sum(axis=0))\n",
        "print('\\nClasses in validation set:')\n",
        "print(target_val.sum(axis=0))"
      ],
      "metadata": {
        "id": "_dFKlhDT_t5_"
      },
      "id": "_dFKlhDT_t5_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 2) DATA PREPROCESSING: FEATURE NORMALIZATION\n",
        "\n",
        "#calculating features averages and std devs\n",
        "avg = features_train.mean()\n",
        "std = features_train.std()\n",
        "\n",
        "#standardizing the data (mean 0, std 1)\n",
        "features_train = (features_train - avg)/std\n",
        "features_val = (features_val - avg)/std"
      ],
      "metadata": {
        "id": "9AX5K66hD_QW"
      },
      "id": "9AX5K66hD_QW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3) SET THE HYPERPARAMETERS\n",
        "input_shape = (features_train.shape[1],) ## tuple that specifies the number of features\n",
        "hidden_nodes_1 = 8\n",
        "hidden_nodes_2 = 5\n",
        "hidden_activation_1 = 'relu'\n",
        "hidden_activation_2 = 'relu'\n",
        "num_classes = target_val.shape[1] ## number of columns in OHE target array\n",
        "output_activation = 'softmax'\n",
        "loss_function = 'categorical_crossentropy'\n",
        "optimizer_used = 'rmsprop' ## Root Mean Square Propagation\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "pz2upQW1_zPr"
      },
      "id": "pz2upQW1_zPr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4) BUILD THE NEURAL NETWORK MODEL\n",
        "\n",
        "## resetting the seed (new model graph by tensorflow: seed needs to be specified again)\n",
        "def reset_random_seeds(nseed, enable_determinism=False):\n",
        "    tf.keras.utils.set_random_seed(nseed)\n",
        "    #np.random.seed(n2)\n",
        "    if enable_determinism:\n",
        "        tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "reset_random_seeds(19)\n",
        "\n",
        "# binary classification shallow neural network model in Keras\n",
        "model = Sequential()\n",
        "model.add(tf.keras.Input(input_shape))\n",
        "model.add(Dense(units=hidden_nodes_1, activation=hidden_activation_1))\n",
        "model.add(Dense(units=hidden_nodes_2, activation=hidden_activation_2))\n",
        "model.add(Dense(num_classes, activation=output_activation))"
      ],
      "metadata": {
        "id": "gER4fA2dARdC"
      },
      "id": "gER4fA2dARdC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 5) COMPILE THE MODEL\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function)"
      ],
      "metadata": {
        "id": "6iaBJxGaAWAN"
      },
      "id": "6iaBJxGaAWAN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "-yF297nUAb6B"
      },
      "id": "-yF297nUAb6B",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>Parameters breakdown</u>:\n",
        "- layer 1: 8 x (4 + 1) = 40\n",
        "- layer 2: 5 x (8 + 1) = 45\n",
        "- layer 3: 3 x (5 + 1) = 18\n",
        "- 40 + 45 + 18 = 103 total parameters"
      ],
      "metadata": {
        "id": "KbwXchu2BNag"
      },
      "id": "KbwXchu2BNag"
    },
    {
      "cell_type": "code",
      "source": [
        "## 6) TRAINING THE NEURAL NETWORK\n",
        "\n",
        "start = time.time()\n",
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_val, target_val), verbose=0)\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "CmmUhppEAd2k"
      },
      "id": "CmmUhppEAd2k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 7) MODEL EVALUATION\n",
        "\n",
        "predictions = model.predict(features_val)\n",
        "print(predictions[0:5,:])\n",
        "\n",
        "predicted_classes = np.argmax(predictions,axis=1)\n",
        "predicted_classes = predicted_classes.reshape(len(predicted_classes),1)\n",
        "\n",
        "target_classes = target[val_index]\n",
        "\n",
        "con_mat_df = confusion_matrix(target_classes, predicted_classes, labels = [0,1,2])\n",
        "print(\"\\nConfusion matrix:\")\n",
        "print(con_mat_df)"
      ],
      "metadata": {
        "id": "5BEn9hsEEiEv"
      },
      "id": "5BEn9hsEEiEv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural network model for linear regression\n",
        "\n",
        "We use the same diabetes dataset as in the previous notebook on the one-unit neural network model for linear regression.\n",
        "\n",
        "The dataset has 442 samples (i.e. patients). For each sample we have 10 features (age, sex, blood pressure and so forth). We need to predict a score describing the disease progression (\n",
        "[details here](https://scikit-learn.org/stable/datasets/toy_dataset.html#diabetes-dataset)).\n",
        "The features have already been normalized (`scaled=True`)"
      ],
      "metadata": {
        "id": "8LK1BqaSFCB0"
      },
      "id": "8LK1BqaSFCB0"
    },
    {
      "cell_type": "code",
      "source": [
        "## 0) GET THE DATA\n",
        "\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "#here \"scaled=True\" is already the default, we write it explicitly just to\n",
        "#let you know that it's an existing option\n",
        "X, y = load_diabetes(return_X_y=True, scaled=True)\n",
        "\n",
        "print(\"First 5 rows of the features\\n\")\n",
        "print(X[0:5,:])\n",
        "\n",
        "print(\"\\nFirst 5 target values\\n\")\n",
        "y[0:5]"
      ],
      "metadata": {
        "id": "2MKkK5AuFELr"
      },
      "id": "2MKkK5AuFELr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "After obtaining the data, we do the usual routine:\n",
        "\n",
        "1. split the data\n",
        "2. set the hyperparameters (linear regression: loss function is MSE, activation function in the output node is linear)\n",
        "3. build the model\n",
        "4. compile the model\n",
        "5. train the model\n",
        "6. evaluate the model\n",
        "\n",
        "This time, we build a smaller neural network with two intermediate layers.\n",
        "\n",
        "! Caveat: this is most likely an overkill, i.e. an overparameterized / overcomplicated model for a simple problem that could be better solved in some other ways !"
      ],
      "metadata": {
        "id": "2sTc-Nh_IiHi"
      },
      "id": "2sTc-Nh_IiHi"
    },
    {
      "cell_type": "code",
      "source": [
        "## 1) DATA SPLIT\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, stratify=X[:,1])\n",
        "print(\"Train features shape: \" + str(X_train.shape))\n",
        "print(\"Validation features shape: \" + str(X_val.shape))"
      ],
      "metadata": {
        "id": "3UpoWWUgGYf3"
      },
      "id": "3UpoWWUgGYf3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as keras\n",
        "\n",
        "## 2) SET THE HYPERPARAMETERS\n",
        "input_shape = (X_train.shape[1],) ## tuple that specifies the number of features\n",
        "hidden_nodes_1 = 3\n",
        "hidden_nodes_2 = 3\n",
        "hidden_activation_1 = 'sigmoid'\n",
        "hidden_activation_2 = 'relu'\n",
        "output_activation = 'linear'\n",
        "loss_function = 'mse'\n",
        "optimizer_used = keras.optimizers.SGD(learning_rate=0.001)\n",
        "num_epochs = 100"
      ],
      "metadata": {
        "id": "l3G2A9wGHUyq"
      },
      "id": "l3G2A9wGHUyq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 3) BUILD THE NEURAL NETWORK MODEL\n",
        "\n",
        "## resetting the seed (new model graph by tensorflow: seed needs to be specified again)\n",
        "def reset_random_seeds(nseed, enable_determinism=False):\n",
        "    tf.keras.utils.set_random_seed(nseed)\n",
        "    if enable_determinism:\n",
        "        tf.config.experimental.enable_op_determinism()\n",
        "\n",
        "reset_random_seeds(77)\n",
        "\n",
        "# binary classification shallow neural network model in Keras\n",
        "model = Sequential()\n",
        "model.add(tf.keras.Input(input_shape))\n",
        "model.add(Dense(units=hidden_nodes_1, activation=hidden_activation_1))\n",
        "model.add(Dense(units=hidden_nodes_2, activation=hidden_activation_2))\n",
        "model.add(Dense(1, activation=output_activation))"
      ],
      "metadata": {
        "id": "nqLsIsMFI_-j"
      },
      "id": "nqLsIsMFI_-j",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 4) COMPILE THE MODEL\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function)"
      ],
      "metadata": {
        "id": "SiutcadwJKQW"
      },
      "id": "SiutcadwJKQW",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "ZLR-bzYMJNF7"
      },
      "id": "ZLR-bzYMJNF7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<u>Parameters breakdown</u>:\n",
        "- layer 1: 3 x (10 + 1) = 33\n",
        "- layer 2: 3 x (3 + 1) = 12\n",
        "- layer 3: 1 x (3 + 1) = 4\n",
        "\n",
        "22 + 12 + 4 = 49 total parameters"
      ],
      "metadata": {
        "id": "OKVVwQlfMILP"
      },
      "id": "OKVVwQlfMILP"
    },
    {
      "cell_type": "code",
      "source": [
        "## 5) TRAINING THE NEURAL NETWORK\n",
        "\n",
        "start = time.time()\n",
        "h = model.fit(x=X_train, y=y_train, epochs=num_epochs, verbose=0, validation_data=(X_val, y_val))\n",
        "end = time.time()\n",
        "print(end - start)"
      ],
      "metadata": {
        "id": "BZVbhIE7JPKF"
      },
      "id": "BZVbhIE7JPKF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_history(history, 'Logistic (' + str(num_epochs) + ' epochs)')"
      ],
      "metadata": {
        "id": "bEO1JXwgJtqV"
      },
      "id": "bEO1JXwgJtqV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_val[0:5,:]"
      ],
      "metadata": {
        "id": "D2aabKP0MVJH"
      },
      "id": "D2aabKP0MVJH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#predicting\n",
        "y_val_pred = model.predict(X_val)\n",
        "y_train_pred = model.predict(X_train)\n",
        "\n",
        "#plotting true vs predicted values\n",
        "plt.plot(y_train, y_train_pred, 'o', label='Train')\n",
        "plt.plot(y_val, y_val_pred, 'o', label='Validation')\n",
        "plt.xlabel(\"Ground truth\")\n",
        "plt.ylabel(\"Predictions\")\n",
        "\n",
        "#done\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9OVUeHPeJZdD"
      },
      "id": "9OVUeHPeJZdD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "\n",
        "print(f\"Mean squared error: {mean_squared_error(y_val, y_val_pred):.2f}\")\n",
        "print(f\"Coefficient of determination: {r2_score(y_val, y_val_pred):.2f}\")\n",
        "\n",
        "res = np.corrcoef(y_val.T,y_val_pred.T)\n",
        "print(f\"Pearson correlation coefficient: {res[0,1]:.2f}\")"
      ],
      "metadata": {
        "id": "HxYf6OdjJkHK"
      },
      "id": "HxYf6OdjJkHK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Conclusions\n",
        "\n",
        "This notebook showed how to build a multi-layers neural network for a linear regression problem (continuous target variable to be predicted from some tabular data).\n",
        "This is not usually a good idea: the neural network requires more computational power, more code, and it's performance are equivalent (at best!) to those of simpler regression models (oftentimes the performance is worse: overfitting).\n",
        "\n",
        "Neural networks shine on more complex problem, where linear regression is not a viable alternative (e.g. complex data structures, non-tabular data, very large datasets, non-linear relationships between the variables etc.)."
      ],
      "metadata": {
        "id": "mCxMLzocUWWt"
      },
      "id": "mCxMLzocUWWt"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}