{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "16f2f29e",
      "metadata": {
        "id": "16f2f29e"
      },
      "source": [
        "# Basic dense (fully connected) neural network models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic simple neural network model\n",
        "\n",
        "In the previous notebooks, we saw how to implement simple neural network models with **just the output layer** for logistic, softmax (multiclass) and linear regression problems: this output layer had **only one node** (logistic and linear regression) which performed both the linear combination of input variables + bias and the sigmoid/linear activation:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1PRc719uT1kOUuCMbpHML2sEk7qp6UJnm\">\n",
        "\n",
        "(Softmax regression is slightly different: the single output layer has as many nodes as there are classes, each calculating the linear combination of input variables and the softmax activation).\n"
      ],
      "metadata": {
        "id": "cgmleXAaWYWB"
      },
      "id": "cgmleXAaWYWB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic dense neural network model\n",
        "\n",
        "We are now building a **neural network model**, by adding **one hidden layer** (not deep) with **u nodes** (units):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QROz9pFnMoqTeqrFbele8pFz8qXDSckq\">\n",
        "\n",
        "There's a number of `hyperparameters`:\n",
        "\n",
        "- the **number of hidden nodes** (number of units in the hidden layer)\n",
        "- the **type of activation function** in the hidden layer\n",
        "- the **output activation function**\n",
        "- the **loss function** (for backpropagation)\n",
        "- the **optimizer** (for gradient descent)\n",
        "\n",
        "By stacking together more than one hidden/intermediate layer (additional hyperparameter), we can then build **deep neural networks**."
      ],
      "metadata": {
        "id": "xQl8moH7c_q2"
      },
      "id": "xQl8moH7c_q2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries and setting the random seed\n",
        "\n",
        "First of all, we load some necessary libraries; then we setup the random seed to ensure reproducibility of results. Since tensorflow uses an internal random generator we need to fix both the general seed (via numpy `seed()`) and tensorflow seed (via `set_seet()`)"
      ],
      "metadata": {
        "id": "5Z63YlH0OgfH"
      },
      "id": "5Z63YlH0OgfH"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "9e86c1bf",
      "metadata": {
        "id": "9e86c1bf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "  # Set the seed using keras.utils.set_random_seed. This will set:\n",
        "  # 1) `numpy` seed\n",
        "  # 2) `tensorflow` random seed\n",
        "  # 3) `python` random seed\n",
        "tf.keras.utils.set_random_seed(10)\n",
        "\n",
        "  # This will make TensorFlow ops as deterministic as possible, but it will\n",
        "  # affect the overall performance, so it's not enabled by default.\n",
        "  # `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data\n",
        "\n",
        "Example data"
      ],
      "metadata": {
        "id": "zZ3vzxq4B0IQ"
      },
      "id": "zZ3vzxq4B0IQ"
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "09318d92",
      "metadata": {
        "id": "09318d92",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7dcd5b5-2d46-4b67-930b-dbdbe9fe282b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(150, 4)\n",
            "(150,)\n"
          ]
        }
      ],
      "source": [
        "import sklearn.datasets\n",
        "\n",
        "(features, target) = sklearn.datasets.load_iris(return_X_y = True) ## feature names are not returned\n",
        "print(features.shape)\n",
        "print(target.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## # Configuration options\n",
        "input_shape = (features.shape[1],) ## tuple that specifies the number of features\n",
        "hidden_nodes = 8\n",
        "hidden_activation = 'relu'\n",
        "output_activation = 'sigmoid'\n",
        "loss_function = 'binary_crossentropy'\n",
        "optimizer_used = 'SGD' ##stochastic gradient descent\n",
        "num_epochs = 200"
      ],
      "metadata": {
        "id": "7bfjgppxe8Oe"
      },
      "id": "7bfjgppxe8Oe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[TODO]"
      ],
      "metadata": {
        "id": "Nz8nRg1_HyjY"
      },
      "id": "Nz8nRg1_HyjY"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}