{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "16f2f29e",
      "metadata": {
        "id": "16f2f29e"
      },
      "source": [
        "# Basic dense (fully connected) neural network models\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic simple neural network model\n",
        "\n",
        "In the previous notebooks, we saw how to implement simple neural network models with **just the output layer** for logistic, softmax (multiclass) and linear regression problems: this output layer had **only one node** (logistic and linear regression) which performed both the linear combination of input variables + bias and the sigmoid/linear activation:\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1PRc719uT1kOUuCMbpHML2sEk7qp6UJnm\">\n",
        "\n",
        "(Softmax regression is slightly different: the single output layer has as many nodes as there are classes, each calculating the linear combination of input variables and the softmax activation).\n"
      ],
      "metadata": {
        "id": "cgmleXAaWYWB"
      },
      "id": "cgmleXAaWYWB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic dense neural network model\n",
        "\n",
        "We are now building a **neural network model**, by adding **one hidden layer** (not deep) with **u nodes** (units):\n",
        "\n",
        "<img src=\"https://drive.google.com/uc?export=view&id=1QROz9pFnMoqTeqrFbele8pFz8qXDSckq\">\n",
        "\n",
        "There's a number of `hyperparameters`:\n",
        "\n",
        "- the **number of hidden nodes** (number of units in the hidden layer)\n",
        "- the **type of activation function** in the hidden layer\n",
        "- the **output activation function**\n",
        "- the **loss function** (for backpropagation)\n",
        "- the **optimizer** (for gradient descent)\n",
        "\n",
        "By stacking together more than one hidden/intermediate layer (additional hyperparameter), we can then build **deep neural networks**."
      ],
      "metadata": {
        "id": "xQl8moH7c_q2"
      },
      "id": "xQl8moH7c_q2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries and setting the random seed\n",
        "\n",
        "First of all, we load some necessary libraries; then we setup the random seed to ensure reproducibility of results. Since tensorflow uses an internal random generator we need to fix both the general seed (via numpy `seed()`) and tensorflow seed (via `set_seet()`)"
      ],
      "metadata": {
        "id": "5Z63YlH0OgfH"
      },
      "id": "5Z63YlH0OgfH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e86c1bf",
      "metadata": {
        "id": "9e86c1bf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "  # Set the seed using keras.utils.set_random_seed. This will set:\n",
        "  # 1) `numpy` seed\n",
        "  # 2) `tensorflow` random seed\n",
        "  # 3) `python` random seed\n",
        "tf.keras.utils.set_random_seed(10)\n",
        "\n",
        "  # This will make TensorFlow ops as deterministic as possible, but it will\n",
        "  # affect the overall performance, so it's not enabled by default.\n",
        "  # `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data\n",
        "\n",
        "We get the usual `iris` dataset:"
      ],
      "metadata": {
        "id": "zZ3vzxq4B0IQ"
      },
      "id": "zZ3vzxq4B0IQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09318d92",
      "metadata": {
        "id": "09318d92"
      },
      "outputs": [],
      "source": [
        "import sklearn.datasets\n",
        "\n",
        "(features, target) = sklearn.datasets.load_iris(return_X_y = True) ## feature names are not returned\n",
        "print(features.shape)\n",
        "print(target.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a three-class problem, and for the logistic regression example we need to binarise it:"
      ],
      "metadata": {
        "id": "wBxq3lGLu00F"
      },
      "id": "wBxq3lGLu00F"
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(target, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "metadata": {
        "id": "n0I1rUV9uWdI"
      },
      "id": "n0I1rUV9uWdI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#updating class labels. To makes things difficult we put together old classes 0 and 1\n",
        "#in a new class (non virginica) and keep old class 2 (virginica) as new class 1.\n",
        "#For an easier problems put together versicolor and virginica and keep setosa by itself\n",
        "j = 100 ## split: 50 for setosa vs versicolor+virginica, 100 for setosa+versicolor vs virginica\n",
        "binary_target = np.copy(target)\n",
        "binary_target[0:j] = 0\n",
        "binary_target[j:150] = 1"
      ],
      "metadata": {
        "id": "tTyIWlhLuTB4"
      },
      "id": "tTyIWlhLuTB4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "unique, counts = np.unique(binary_target, return_counts=True)\n",
        "print(np.asarray((unique, counts)).T)"
      ],
      "metadata": {
        "id": "qZNIOp89vbMQ"
      },
      "id": "qZNIOp89vbMQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and validation sets"
      ],
      "metadata": {
        "id": "Nz8nRg1_HyjY"
      },
      "id": "Nz8nRg1_HyjY"
    },
    {
      "cell_type": "code",
      "source": [
        "#we want to have the same proportion of classes in both train and validation sets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n",
        "#assigned to validation set (here called \"test\")\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "\n",
        "#the .split() method returns (an iterable over) two lists which can be\n",
        "#used to index the samples that go into train and validation sets\n",
        "for train_index, val_index in sss.split(features, binary_target):\n",
        "    features_train = features[train_index, :]\n",
        "    features_val   = features[val_index, :]\n",
        "    target_train   = binary_target[train_index]\n",
        "    target_val     = binary_target[val_index]\n",
        "\n",
        "#let's print some shapes to get an idea of the resulting data structure\n",
        "print(features_train.shape)\n",
        "print(features_val.shape)\n",
        "print(target_train.shape)\n",
        "print(target_val.shape)"
      ],
      "metadata": {
        "id": "kCZCcaW3uBgg"
      },
      "id": "kCZCcaW3uBgg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "\n",
        "print(Counter(target_train))\n",
        "print(Counter(target_val))"
      ],
      "metadata": {
        "id": "HVoTq9-twsIF"
      },
      "id": "HVoTq9-twsIF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target_train"
      ],
      "metadata": {
        "id": "5A6QNY5k0AYs"
      },
      "id": "5A6QNY5k0AYs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the neural network model\n",
        "\n",
        "We now build our neural network for binary classification: it will be comprised of one intermediate layer and one output layer which that will perform the final classification (actually, the calculation of the probability of belonging to class `1` given the input features: $P(y=1|x$)).\n",
        "\n",
        "The necessary steps are:\n",
        "\n",
        "- model set-up (define the hyperparameters)\n",
        "- model architecture\n",
        "- compiling (putting together the configuration -model set-up- and the architecture)"
      ],
      "metadata": {
        "id": "vjefipM6w5rq"
      },
      "id": "vjefipM6w5rq"
    },
    {
      "cell_type": "code",
      "source": [
        "## # Configuration options\n",
        "input_shape = (features.shape[1],) ## tuple that specifies the number of features\n",
        "hidden_nodes = 8\n",
        "hidden_activation = 'relu'\n",
        "output_activation = 'sigmoid'\n",
        "loss_function = 'binary_crossentropy'\n",
        "optimizer_used = 'SGD' ##stochastic gradient descent\n",
        "num_epochs = 200"
      ],
      "metadata": {
        "id": "7bfjgppxe8Oe"
      },
      "id": "7bfjgppxe8Oe",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#we are building a \"sequential\" model, meaning that the data will\n",
        "#flow like INPUT -> ELABORATION -> OUTPUT. In particular, we will\n",
        "#not have any loops, i.e. our output will never be recycled as\n",
        "#input for the first layer\n",
        "from keras.models import Sequential\n",
        "\n",
        "#a \"dense\" layer is a layer were all the data coming in are connected\n",
        "#to all nodes (fully connected).\n",
        "from keras.layers import Dense, Input\n",
        "\n",
        "# 2-class logistic regression in Keras\n",
        "model = Sequential()\n",
        "model.add(Input(input_shape))\n",
        "model.add(Dense(units=hidden_nodes, activation=hidden_activation))\n",
        "model.add(Dense(units=1, activation=output_activation))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model.compile(optimizer=optimizer_used, loss=loss_function)"
      ],
      "metadata": {
        "id": "7-OPng9vwsfr"
      },
      "id": "7-OPng9vwsfr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "U-R-j2XCyEdh"
      },
      "id": "U-R-j2XCyEdh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `summary()` method of the Keras model tells us that there are 49  parameters to train:\n",
        "- w1, w2, w3, w4, b (weights for the 4 features + bias term) for each of the 8 nodes in the hidden layer ($\\rightarrow$ (4+1) x 8 = 40 parameters);\n",
        "- w1 - w8 + b: weights for the results from the 8 intermediate nodes (\"new features\") + bias term, for the output layer ($\\rightarrow$ 8 + 1 = 9 parameters)\n",
        "- layer 1 (40 parameters) + layer 2 (9 parameters) = 49 total parameters"
      ],
      "metadata": {
        "id": "xwCbMT88yi9T"
      },
      "id": "xwCbMT88yi9T"
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(features_train, target_train, epochs=num_epochs, validation_data=(features_val, target_val), verbose=1)"
      ],
      "metadata": {
        "id": "q1VSjPS_yrGO"
      },
      "id": "q1VSjPS_yrGO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to take a look at losses evolution\n",
        "def plot_loss_history(h, title):\n",
        "    plt.plot(h.history['loss'], label = \"Train loss\")\n",
        "    plt.plot(h.history['val_loss'], label = \"Validation loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "T1suZSVVzhSO"
      },
      "id": "T1suZSVVzhSO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_history(history, 'Logistic (10 epochs)')"
      ],
      "metadata": {
        "id": "o0wcznnUzjHd"
      },
      "id": "o0wcznnUzjHd",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}