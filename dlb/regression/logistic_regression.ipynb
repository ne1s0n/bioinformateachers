{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "16f2f29e",
      "metadata": {
        "id": "16f2f29e"
      },
      "source": [
        "## One-unit neural network model for logistic regression\n",
        "\n",
        "In this notebook, we illustrate how to do [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
        "using a neural network-like implementation with one single unit (perceptron).\n",
        "This is equivalent to a logistic regression model, only it is solved using neural networks (forward and backpropagation) rather than with maximum likelihood (or similar algorithms).\n",
        "\n",
        "As example data, we use the famous [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6deb0024",
      "metadata": {
        "id": "6deb0024"
      },
      "source": [
        "Basically, a neural network implementation of logistic regression will look like the sketch below: first, the vector of input features $\\mathbf{x}$ is multiplied by the vector of weights $\\mathbf{w}$, the results are summed up together and the bias term $b$ is added. This will return the real-valued variable $z$ in the interval $[\\pm âˆž]$.\n",
        "Then, $z$ is activated with the logistic (sigmoid) function $\\rightarrow \\sigma(z)$ to give $P(y=1|x)$, the probability of belonging to class `1` given the input features $x$.\n",
        "\n",
        "<img src=\"logistic_perceptron.png\" alt=\"perceptron\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries and setting the random seed\n",
        "\n",
        "First of all, we load some necessary libraries; then we setup the random seed to ensure reproducibility of results. Since tensorflow uses an internal random generator we need to fix both the general seed (via numpy `seed()`) and tensorflow seed (via `set_seet()`)"
      ],
      "metadata": {
        "id": "5Z63YlH0OgfH"
      },
      "id": "5Z63YlH0OgfH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e86c1bf",
      "metadata": {
        "id": "9e86c1bf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "  # Set the seed using keras.utils.set_random_seed. This will set:\n",
        "  # 1) `numpy` seed\n",
        "  # 2) `tensorflow` random seed\n",
        "  # 3) `python` random seed\n",
        "tf.keras.utils.set_random_seed(10)\n",
        "\n",
        "  # This will make TensorFlow ops as deterministic as possible, but it will\n",
        "  # affect the overall performance, so it's not enabled by default.\n",
        "  # `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data\n",
        "\n",
        "From `sklearn.datasets` there are generally two ways to import the data:\n",
        "\n",
        "1. `return_X_y = False` (default): returns a \"bunch\" object that contains both the `target` and the `features` (to be accessed as attributes): `<dataset>.target` and `<dataset>.data`\n",
        "2. `return_X_y = True`: returns directly the target and features separately"
      ],
      "metadata": {
        "id": "zZ3vzxq4B0IQ"
      },
      "id": "zZ3vzxq4B0IQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09318d92",
      "metadata": {
        "id": "09318d92"
      },
      "outputs": [],
      "source": [
        "import sklearn.datasets\n",
        "\n",
        "## 1)\n",
        "iris = sklearn.datasets.load_iris()\n",
        "features = iris.data\n",
        "target = iris.target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2)\n",
        "(features_, target_) = sklearn.datasets.load_iris(return_X_y = True) ## feature names are not returned"
      ],
      "metadata": {
        "id": "2grViWP-Cze8"
      },
      "id": "2grViWP-Cze8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target == target_"
      ],
      "metadata": {
        "id": "FQGnQL1eDOsx"
      },
      "id": "FQGnQL1eDOsx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we convert data and features (originally as `numpy` arrays) to `pandas` dataframes / series"
      ],
      "metadata": {
        "id": "1rStMQUKDmt-"
      },
      "id": "1rStMQUKDmt-"
    },
    {
      "cell_type": "code",
      "source": [
        "iris.data = pd.DataFrame(features, columns=iris.feature_names) #converting numpy array -> pandas DataFrame\n",
        "iris.target = pd.Series(target) #converting numpy array -> pandas Series"
      ],
      "metadata": {
        "id": "6jEcpPgtDDYh"
      },
      "id": "6jEcpPgtDDYh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **iris** dataset is a historic dataset first used by **Ronald Fisher** in his 1936 paper on linear discriminant analysis (LDA).\n",
        "The dataset contains information on 150 flower samples, belonging to three species of Iris (Iris setosa, Iris virginica and Iris versicolor: the `target`).\n",
        "Four `features` were measured from each sample: the length and the width of the sepals and of the petals, in centimeters."
      ],
      "metadata": {
        "id": "Bpq4XO6UI_ta"
      },
      "id": "Bpq4XO6UI_ta"
    },
    {
      "cell_type": "code",
      "source": [
        "print(iris.DESCR)"
      ],
      "metadata": {
        "id": "zPHqRATEErQ-"
      },
      "id": "zPHqRATEErQ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "\n",
        "As said, the feature data has 150 rown and 4 columns:"
      ],
      "metadata": {
        "id": "FQp5ITO0KXKF"
      },
      "id": "FQp5ITO0KXKF"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of the feature table: ' + str(iris.data.shape))"
      ],
      "metadata": {
        "id": "BaEeW57GKZ9E"
      },
      "id": "BaEeW57GKZ9E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The four features are real-valued numbers in the $10^1$ range:"
      ],
      "metadata": {
        "id": "5aduLrMQK8cG"
      },
      "id": "5aduLrMQK8cG"
    },
    {
      "cell_type": "code",
      "source": [
        "iris.data.describe()"
      ],
      "metadata": {
        "id": "LQPPnwo3KvYE"
      },
      "id": "LQPPnwo3KvYE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `target` is a vector (`pandas` series) of integers (0, 1, 2), one for each flower species (50 samples each):"
      ],
      "metadata": {
        "id": "Gr_okx7eLJ_t"
      },
      "id": "Gr_okx7eLJ_t"
    },
    {
      "cell_type": "code",
      "source": [
        "#using Counter object to print a tally of the classes\n",
        "from collections import Counter\n",
        "print('Numerosity for each class: ' + str(Counter(iris.target)))"
      ],
      "metadata": {
        "id": "dZme8IZ0LF6R"
      },
      "id": "dZme8IZ0LF6R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classes are represented via a numeric index: 0 for *setosa*, 1 for *versicolor*, 2 for *virginica*. The samples are in order: the first 50 samples are *setosa*, then 50 *versicolor* and the last 50 are *virginica*.\n",
        "\n",
        "When working with a new dataset, it is always importat to plot the data. We are unfortunately talking about a 5-dimensional dataset (the four features + the target class) which is not easily representable.\n",
        "One possibility is to take a slice (a subset) of the whole dataset.\n",
        "\n",
        "In the next code chunk we plot two features plus the class."
      ],
      "metadata": {
        "id": "OBMUK3_NL5bI"
      },
      "id": "OBMUK3_NL5bI"
    },
    {
      "cell_type": "code",
      "source": [
        "## SELECT FEATURES TO PLOT\n",
        "#change these two values to plot different features, remembering the numbering:\n",
        "# 0 : sepal length (cm)\n",
        "# 1 : sepal width (cm)\n",
        "# 2 : petal length (cm)\n",
        "# 3 : petal width (cm)\n",
        "feature_x = 0\n",
        "feature_y = 1\n",
        "\n",
        "#starting a new plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "#adding data in three bunches of 50, once per class\n",
        "ax.scatter(x=iris.data.iloc[0:50,feature_x],    y=iris.data.iloc[0:50,feature_y],    c='red',   label=iris.target_names[0])\n",
        "ax.scatter(x=iris.data.iloc[50:100,feature_x],  y=iris.data.iloc[50:100,feature_y],  c='green', label=iris.target_names[1])\n",
        "ax.scatter(x=iris.data.iloc[100:150,feature_x], y=iris.data.iloc[100:150,feature_y], c='blue',  label=iris.target_names[2])\n",
        "\n",
        "#the axis names are taken from feature names\n",
        "ax.set_xlabel(iris.feature_names[feature_x])\n",
        "ax.set_ylabel(iris.feature_names[feature_y])\n",
        "\n",
        "#adding the legend and printing the plot\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KrqLvYQWNav_"
      },
      "id": "KrqLvYQWNav_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows clearly that setosa is quite separate from the other two classes. Whichever features you choose for the plot, you more or less get this same general result.\n",
        "\n",
        "This is however a **three-class** problem: however, we want to build a **neural network model** for **logistic regression** applied to a **binary classification problem**.\n",
        "\n",
        "Therefore, we need some preparatory ('preprocessing') steps."
      ],
      "metadata": {
        "id": "PawFSrXlNwe_"
      },
      "id": "PawFSrXlNwe_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "\n",
        "We need to transform a **three-class problem in a two-class problem**: we choose to **join together Setosa and Versicolor** (the red and green dots from the plot above), which appear to be less obviously separable based on sepal (and petal) length and width (and hence promise to provide more of a challenge to our neural network problem).\n",
        "In other words, we want to build a classifier able to discriminate `virginica` (which becomes the new class \"1\") from the other `non-virginica` flowers (which all together become the new class \"0\").\n",
        "\n",
        "To make the problem a little more challenging (and interesting) we decide to **drop half of the features**, **using only the first two** columns."
      ],
      "metadata": {
        "id": "zD4xK76sOajn"
      },
      "id": "zD4xK76sOajn"
    },
    {
      "cell_type": "code",
      "source": [
        "#simplifly the problem: less classes, less features\n",
        "features = iris.data.iloc[:, 0:2]\n",
        "target = iris.target\n",
        "\n",
        "#updating class labels. To makes things difficult we put together old classes 0 and 1\n",
        "#in a new class (non virginica) and keep old class 2 (virginica) as new class 1.\n",
        "#For an easier problems put together versicolor and virginica and keep setosa by itself\n",
        "j = 100 ## split: 50 for setosa vs versicolor+virginica, 100 for setosa+versicolor vs virginica\n",
        "target[0:j] = 0\n",
        "target[j:150] = 1"
      ],
      "metadata": {
        "id": "SfSXs5pZOaDO"
      },
      "id": "SfSXs5pZOaDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#starting a new plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "#adding data in two bunches\n",
        "ax.scatter(x=features.iloc[0:j,0],   y=features.iloc[0:j,1],   c='red',  label='non-virginica')\n",
        "ax.scatter(x=features.iloc[j:150,0], y=features.iloc[j:150,1], c='blue', label='virginica')\n",
        "\n",
        "#the axis names are taken from feature names\n",
        "ax.set_xlabel(iris.feature_names[feature_x])\n",
        "ax.set_ylabel(iris.feature_names[feature_y])\n",
        "\n",
        "# displaying the title\n",
        "plt.title(\"Two-class simplified problem\")\n",
        "\n",
        "#adding the legend and printing the plot\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "htJv49_hSXN6"
      },
      "id": "htJv49_hSXN6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Things are getting interesting! This is now a difficult problem and there is no clear cut solution. Let's proceed."
      ],
      "metadata": {
        "id": "homZQT2rS2AI"
      },
      "id": "homZQT2rS2AI"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training and validation sets\n",
        "\n",
        "Each time there is some kind of \"learning\" involved we need to split our data. A subset will be used for training, and a subset will be used for validation.\n",
        "\n",
        "In our current dataset the samples are sorted by class: the first 100 are \"non-virginica\" and the remaining 50 are \"virginica\".\n",
        "We want to keep this 2:1 proportion (roughly) the same in both train and validation set.\n",
        "\n",
        "Obviously (given the ordered data), taking the first $80\\%$ (120 samples) of the data as training and the rest (30 samples) as validation would be a bad choice ... (only 'virginica' samples in the validation set, 4:1 class proportion in the training set).   \n",
        "\n",
        "Simple random sampling would also not be a good solution, since the 2:1 proportion will not be maintained.\n",
        "\n",
        "Therefore we are going to use what is called a [stratified approach](https://machinelearningmastery.com/cross-validation-for-imbalanced-classification/) using a [StratifiedShuffleSplit](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.StratifiedShuffleSplit.html) object from `scikit-learn`:\n"
      ],
      "metadata": {
        "id": "K98ZAN9-S4a8"
      },
      "id": "K98ZAN9-S4a8"
    },
    {
      "cell_type": "code",
      "source": [
        "#we want to have the same proportion of classes in both train and validation sets\n",
        "from sklearn.model_selection import StratifiedShuffleSplit\n",
        "\n",
        "#building a StratifiedShuffleSplit object (sss among friends) with 20% data\n",
        "#assigned to validation set (here called \"test\")\n",
        "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=0)\n",
        "\n",
        "#the .split() method returns (an iterable over) two lists which can be\n",
        "#used to index the samples that go into train and validation sets\n",
        "for train_index, val_index in sss.split(features, target):\n",
        "    features_train = features.iloc[train_index, :]\n",
        "    features_val   = features.iloc[val_index, :]\n",
        "    target_train   = target[train_index]\n",
        "    target_val     = target[val_index]\n",
        "\n",
        "#let's print some shapes to get an idea of the resulting data structure\n",
        "print(features_train.shape)\n",
        "print(features_val.shape)\n",
        "print(target_train.shape)\n",
        "print(target_val.shape)"
      ],
      "metadata": {
        "id": "Zz1Miz6TSw84"
      },
      "id": "Zz1Miz6TSw84",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(Counter(target_train))\n",
        "print(Counter(target_val))"
      ],
      "metadata": {
        "id": "IAMFHDFIVD1L"
      },
      "id": "IAMFHDFIVD1L",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that we have $80\\%$ data ready for training and $20\\%$ for validation, both sets with a 2:1 target class ratio: job done!"
      ],
      "metadata": {
        "id": "mki_AVWuVGjh"
      },
      "id": "mki_AVWuVGjh"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the neural network model\n",
        "\n",
        "We want to use a neural network model for logistic regression.\n",
        "There are many `Python` frameworks that implement (Deep) Neural Networks: here we are using `Keras` (for more details on how to use *keras* a good starting point is the [documentation on training and evaluation](https://www.tensorflow.org/guide/keras/train_and_evaluate)).\n",
        "\n",
        "Our neural network will be very minimal, as illustrated in the sketch at the beginning of this notebook: it will be comprised of only one node (neuron) that will perform both 1) the linear combination of weighted input variables + bias term; and 2) activate the result of step 1 with the sigmoid function to produce the probability of belonging to class \"1\" (or \"0\").\n",
        "\n",
        "We are now ready to build the NN (neural network) model!\n",
        "\n",
        "- model set-up\n",
        "- model architecture and compiling"
      ],
      "metadata": {
        "id": "ecoydX0bVgMx"
      },
      "id": "ecoydX0bVgMx"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model set-up\n",
        "\n",
        "We first need to define some hyperparameters:\n",
        "\n",
        "1. activation function (`sigmoid`, this is a binary classification problem)\n",
        "2. optimizer (the algorithm that carries out forward and backpropagation to solve the model)\n",
        "3. the loss function (`binary cross-entropy`, again this is binary classification!)"
      ],
      "metadata": {
        "id": "JHec8oN7cxvy"
      },
      "id": "JHec8oN7cxvy"
    },
    {
      "cell_type": "code",
      "source": [
        "activation_function = 'sigmoid'\n",
        "optimizing_method = 'rmsprop'\n",
        "loss_function = 'binary_crossentropy'"
      ],
      "metadata": {
        "id": "_mPrv77YVSwo"
      },
      "id": "_mPrv77YVSwo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model architecture\n",
        "\n",
        "This is a one-layer one-unit neural network model.\n",
        "The neural network is fully connected, meaning that the unit receives in input all features."
      ],
      "metadata": {
        "id": "HAbTw4oDdSMS"
      },
      "id": "HAbTw4oDdSMS"
    },
    {
      "cell_type": "code",
      "source": [
        "#we are building a \"sequential\" model, meaning that the data will\n",
        "#flow like INPUT -> ELABORATION -> OUTPUT. In particular, we will\n",
        "#not have any loops, i.e. our output will never be recycled as\n",
        "#input for the first layer\n",
        "from keras.models import Sequential\n",
        "\n",
        "#a \"dense\" layer is a layer were all the data coming in are connected\n",
        "#to all nodes (fully connected).\n",
        "#In our case there is only one node in the layer, and\n",
        "#it receives all the input features\n",
        "from keras.layers import Dense\n",
        "\n",
        "# 2-class logistic regression in Keras\n",
        "model = Sequential()\n",
        "model.add(Dense(units=1, activation='sigmoid', input_dim=features_train.shape[1]))\n",
        "\n",
        "#the model is declared, but we still need to compile it to actually\n",
        "#build all the data structures\n",
        "model.compile(optimizer=optimizing_method, loss=loss_function)"
      ],
      "metadata": {
        "id": "LlgkOZuudTvT"
      },
      "id": "LlgkOZuudTvT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the `.summary()` method we can take a look inside the model:"
      ],
      "metadata": {
        "id": "Awoe08PBd_HG"
      },
      "id": "Awoe08PBd_HG"
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.summary())"
      ],
      "metadata": {
        "id": "NKOLG-tWd6o5"
      },
      "id": "NKOLG-tWd6o5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We see that there are three trainable parameters (the weights for the two input variables, W1, W2, plus the bias term 'B'), and a single node.\n",
        "The output is a single number. Excellent."
      ],
      "metadata": {
        "id": "LQlHhEv7eHrC"
      },
      "id": "LQlHhEv7eHrC"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training the neural network model"
      ],
      "metadata": {
        "id": "bMoZEu_phn7j"
      },
      "id": "bMoZEu_phn7j"
    },
    {
      "cell_type": "code",
      "source": [
        "history = model.fit(features_train, target_train, epochs=10, validation_data=(features_val, target_val))"
      ],
      "metadata": {
        "id": "OzRyL84ud7El"
      },
      "id": "OzRyL84ud7El",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#function to take a look at losses evolution\n",
        "def plot_loss_history(h, title):\n",
        "    plt.plot(h.history['loss'], label = \"Train loss\")\n",
        "    plt.plot(h.history['val_loss'], label = \"Validation loss\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "siu0zjjehvFf"
      },
      "id": "siu0zjjehvFf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_history(history, 'Logistic (10 epochs)')"
      ],
      "metadata": {
        "id": "pVKSJX8ShxPh"
      },
      "id": "pVKSJX8ShxPh",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}