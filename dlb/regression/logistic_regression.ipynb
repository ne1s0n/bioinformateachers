{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "16f2f29e",
      "metadata": {
        "id": "16f2f29e"
      },
      "source": [
        "## One-unit neural network model for logistic regression\n",
        "\n",
        "In this notebook, we illustrate how to do [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression)\n",
        "using a neural network-like implementation with one single unit (perceptron).\n",
        "This is equivalent to a logistic regression model, only it is solved using neural networks (forward and backpropagation) rather than with maximum likelihood (or similar algorithms).\n",
        "\n",
        "As example data, we use the famous [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6deb0024",
      "metadata": {
        "id": "6deb0024"
      },
      "source": [
        "Basically, a neural network implementation of logistic regression will look like the sketch below: first, the vector of input features $\\mathbf{x}$ is multiplied by the vector of weights $\\mathbf{w}$, the results are summed up together and the bias term $b$ is added. This will return the real-valued variable $z$ in the interval $[\\pm âˆž]$.\n",
        "Then, $z$ is activated with the logistic (sigmoid) function $\\rightarrow \\sigma(z)$ to give $P(y=1|x)$, the probability of belonging to class `1` given the input features $x$.\n",
        "\n",
        "<img src=\"logistic_perceptron.png\" alt=\"perceptron\" style=\"width: 500px;\"/>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries and setting the random seed\n",
        "\n",
        "First of all, we load some necessary libraries; then we setup the random seed to ensure reproducibility of results. Since tensorflow uses an internal random generator we need to fix both the general seed (via numpy `seed()`) and tensorflow seed (via `set_seet()`)"
      ],
      "metadata": {
        "id": "5Z63YlH0OgfH"
      },
      "id": "5Z63YlH0OgfH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e86c1bf",
      "metadata": {
        "id": "9e86c1bf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "  # Set the seed using keras.utils.set_random_seed. This will set:\n",
        "  # 1) `numpy` seed\n",
        "  # 2) `tensorflow` random seed\n",
        "  # 3) `python` random seed\n",
        "tf.keras.utils.set_random_seed(10)\n",
        "\n",
        "  # This will make TensorFlow ops as deterministic as possible, but it will\n",
        "  # affect the overall performance, so it's not enabled by default.\n",
        "  # `enable_op_determinism()` is introduced in TensorFlow 2.9.\n",
        "tf.config.experimental.enable_op_determinism()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data\n",
        "\n",
        "From `sklearn.datasets` there are generally two ways to import the data:\n",
        "\n",
        "1. `return_X_y = False` (default): returns a \"bunch\" object that contains both the `target` and the `features` (to be accessed as attributes): `<dataset>.target` and `<dataset>.data`\n",
        "2. `return_X_y = True`: returns directly the target and features separately"
      ],
      "metadata": {
        "id": "zZ3vzxq4B0IQ"
      },
      "id": "zZ3vzxq4B0IQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "09318d92",
      "metadata": {
        "id": "09318d92"
      },
      "outputs": [],
      "source": [
        "import sklearn.datasets\n",
        "\n",
        "## 1)\n",
        "iris = sklearn.datasets.load_iris()\n",
        "features = iris.data\n",
        "target = iris.target"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## 2)\n",
        "(features_, target_) = sklearn.datasets.load_iris(return_X_y = True) ## feature names are not returned"
      ],
      "metadata": {
        "id": "2grViWP-Cze8"
      },
      "id": "2grViWP-Cze8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "target == target_"
      ],
      "metadata": {
        "id": "FQGnQL1eDOsx"
      },
      "id": "FQGnQL1eDOsx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next, we convert data and features (originally as `numpy` arrays) to `pandas` dataframes / series"
      ],
      "metadata": {
        "id": "1rStMQUKDmt-"
      },
      "id": "1rStMQUKDmt-"
    },
    {
      "cell_type": "code",
      "source": [
        "iris.data = pd.DataFrame(features, columns=iris.feature_names) #converting numpy array -> pandas DataFrame\n",
        "iris.target = pd.Series(target) #converting numpy array -> pandas Series"
      ],
      "metadata": {
        "id": "6jEcpPgtDDYh"
      },
      "id": "6jEcpPgtDDYh",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **iris** dataset is a historic dataset first used by **Ronald Fisher** in his 1936 paper on linear discriminant analysis (LDA).\n",
        "The dataset contains information on 150 flower samples, belonging to three species of Iris (Iris setosa, Iris virginica and Iris versicolor: the `target`).\n",
        "Four `features` were measured from each sample: the length and the width of the sepals and of the petals, in centimeters."
      ],
      "metadata": {
        "id": "Bpq4XO6UI_ta"
      },
      "id": "Bpq4XO6UI_ta"
    },
    {
      "cell_type": "code",
      "source": [
        "print(iris.DESCR)"
      ],
      "metadata": {
        "id": "zPHqRATEErQ-"
      },
      "id": "zPHqRATEErQ-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory Data Analysis (EDA)\n",
        "\n",
        "As said, the feature data has 150 rown and 4 columns:"
      ],
      "metadata": {
        "id": "FQp5ITO0KXKF"
      },
      "id": "FQp5ITO0KXKF"
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of the feature table: ' + str(iris.data.shape))"
      ],
      "metadata": {
        "id": "BaEeW57GKZ9E"
      },
      "id": "BaEeW57GKZ9E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The four features are real-valued numbers in the $10^1$ range:"
      ],
      "metadata": {
        "id": "5aduLrMQK8cG"
      },
      "id": "5aduLrMQK8cG"
    },
    {
      "cell_type": "code",
      "source": [
        "iris.data.describe()"
      ],
      "metadata": {
        "id": "LQPPnwo3KvYE"
      },
      "id": "LQPPnwo3KvYE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The `target` is a vector (`pandas` series) of integers (0, 1, 2), one for each flower species (50 samples each):"
      ],
      "metadata": {
        "id": "Gr_okx7eLJ_t"
      },
      "id": "Gr_okx7eLJ_t"
    },
    {
      "cell_type": "code",
      "source": [
        "#using Counter object to print a tally of the classes\n",
        "from collections import Counter\n",
        "print('Numerosity for each class: ' + str(Counter(iris.target)))"
      ],
      "metadata": {
        "id": "dZme8IZ0LF6R"
      },
      "id": "dZme8IZ0LF6R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Classes are represented via a numeric index: 0 for *setosa*, 1 for *versicolor*, 2 for *virginica*. The samples are in order: the first 50 samples are *setosa*, then 50 *versicolor* and the last 50 are *virginica*.\n",
        "\n",
        "When working with a new dataset, it is always importat to plot the data. We are unfortunately talking about a 5-dimensional dataset (the four features + the target class) which is not easily representable.\n",
        "One possibility is to take a slice (a subset) of the whole dataset.\n",
        "\n",
        "In the next code chunk we plot two features plus the class."
      ],
      "metadata": {
        "id": "OBMUK3_NL5bI"
      },
      "id": "OBMUK3_NL5bI"
    },
    {
      "cell_type": "code",
      "source": [
        "## SELECT FEATURES TO PLOT\n",
        "#change these two values to plot different features, remembering the numbering:\n",
        "# 0 : sepal length (cm)\n",
        "# 1 : sepal width (cm)\n",
        "# 2 : petal length (cm)\n",
        "# 3 : petal width (cm)\n",
        "feature_x = 0\n",
        "feature_y = 1\n",
        "\n",
        "#starting a new plot\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "#adding data in three bunches of 50, once per class\n",
        "ax.scatter(x=iris.data.iloc[0:50,feature_x],    y=iris.data.iloc[0:50,feature_y],    c='red',   label=iris.target_names[0])\n",
        "ax.scatter(x=iris.data.iloc[50:100,feature_x],  y=iris.data.iloc[50:100,feature_y],  c='green', label=iris.target_names[1])\n",
        "ax.scatter(x=iris.data.iloc[100:150,feature_x], y=iris.data.iloc[100:150,feature_y], c='blue',  label=iris.target_names[2])\n",
        "\n",
        "#the axis names are taken from feature names\n",
        "ax.set_xlabel(iris.feature_names[feature_x])\n",
        "ax.set_ylabel(iris.feature_names[feature_y])\n",
        "\n",
        "#adding the legend and printing the plot\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "KrqLvYQWNav_"
      },
      "id": "KrqLvYQWNav_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The plot shows clearly that setosa is quite separate from the other two classes. Whichever features you choose for the plot, you more or less get this same general result.\n",
        "\n",
        "This is however a **three-class** problem: however, we want to build a **neural network model** for **logistic regression** applied to a **binary classification problem**.\n",
        "\n",
        "Therefore, we need some preparatory ('preprocessing') steps."
      ],
      "metadata": {
        "id": "PawFSrXlNwe_"
      },
      "id": "PawFSrXlNwe_"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data preprocessing\n",
        "\n",
        "To make things a little more interesting we decide to **renounce to half of our features**, **using only the first two** columns. Moreover, we **join together Setosa and Versicolor**. In other words, we want a classifier able to discriminate virginica (which becomes the new class \"1\") from the other irises (which all together become the new class \"0\"):"
      ],
      "metadata": {
        "id": "zD4xK76sOajn"
      },
      "id": "zD4xK76sOajn"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SfSXs5pZOaDO"
      },
      "id": "SfSXs5pZOaDO",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}