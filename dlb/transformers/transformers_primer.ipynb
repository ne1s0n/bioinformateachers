{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## A primer on transformers\n",
        "\n",
        "This very basic introduction to self-attention and transformers models in deep learning is structured in the following sections:\n",
        "\n",
        "* [General Introduction](#general-intro)\n",
        "* [Basic attention concept](#attention-concept)\n",
        "* [Detailed attention mechanism](#attention-mechanism)"
      ],
      "metadata": {
        "id": "o7rZ6QAvBfIR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## General intro <a name=\"general-intro\"></a>\n",
        "\n",
        "**Transformers** are a spectacularly effective type of neural network architecture that was introduced with the seminal paper \"**Attention is all you need**\" (Vaswani et al. 2017: [here](https://arxiv.org/abs/1706.03762); largely an effort from scientists at *Google Brain*).\n",
        "\n",
        "- RNN, LSTM, GRU etc.: sequential calculations, no parallelization possible (severe computational limit)\n",
        "- transformers offer a solution to this problem: transformers still capture long-range dependencies in the data (even more so than LSTM/GRU), and at the same time are amenable to parallelization\n",
        "- transformers are an innovative approach that dispenses with recurrence and convolutions entirely\n",
        "\n",
        "Transformers are based on **self-attention** (building block of transformers).\n",
        "**Attention** is an innovative concept that revolutionized deep learning (initially introduced by [Bahdanau et al. 2014](https://arxiv.org/abs/1409.0473))\n",
        "\n",
        "- traditional basic **encoder/decoder** = **embedding layer** + LSTM/GRU units --> single context vector\n",
        "- **attention** adds spearate paths (vectors), one for each input value, from the encoder to the decoder\n",
        "- for each embedded term (vector of continuous numbers on learned features) in the encoder, we can calculate **distances** (e.g. cosine similarities) with terms in the decoder\n",
        "- in this way, the network will give more weight to pairs with higher similarity scores\n",
        "- these similarity scores, transformed through an activation function (usually softmax) and scaled, are the **attention values**\n",
        "- with attention, the encoder basically remains the same (in a simple implementation, it can basically be an embedding layer, maybe + LSTM/GRU)\n",
        "- however, the decoder now has access to the attention values (the separate paths) for each input relative to the output, and these values help predict the decoded output\n",
        "\n",
        "**Attention** $\\rightarrow$ **Cross-Attention** $\\rightarrow$ **Self-Attention** $\\rightarrow$ **Multi-Head Attention**\n",
        "\n",
        "Self-attention refers to an attention mechanism that relates different positions\n",
        "of a single sequence in order to compute a representation of the input data.\n",
        "Self-attention has been used successfully in a variety of tasks including reading comprehension, abstractive summarization, textual entailment and learning task-independent sentence representations."
      ],
      "metadata": {
        "id": "4Lm-9415xi02"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<!-- <img src=\"transformer.png\" alt=\"transformer\" style=\"width: 400px;\"/> -->\n",
        "\n",
        "![](https://drive.google.com/uc?export=view&id=1MTfoDOb0J7EbYOCFotiqnaMbeHnAdASB)\n",
        "\n",
        "<u>Figure above</u>: left, **encoder**: stack of N = 6 layers, each with two\n",
        "sub-layers: i) multi-head self-attention mechanism on input embeddings (e.g. first language vocabulary)), and ii) fully connected feed-forward network, followed by layer normalization. Right, **decoder**: stack of N = 6 layers, each with i) multi-head self-attention on output embeddings (second language vocabulary); ii) multi-head attention over the output of the encoder; iii) fully connected feed-forward network. Similar to the encoder, layer normalization is employed. (multi-head attention refers to the separate attention paths for each input word)"
      ],
      "metadata": {
        "id": "Db1MxKsycpNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A **transformer** is a sequence-to-sequence (image-to-sequence, sequence-to-image, image-to-image etc.) **encoder-decoder model**: a simple transformer is similar to an encoder-decoder RNN model, the only difference being that the RNN layers are replaced with self-attention layers.\n",
        "Both the **encoder** and the **decoder** consist of **multiple layers** of **self-attention** and **feed-forward networks**."
      ],
      "metadata": {
        "id": "Kcv658FZxFyc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **transformers** excel at modeling **sequential data** (NLP) - but also image data etc.\n",
        "- unlike RNNs, **transformers are parallelizable**: the main reasons is that **transformers replace recurrence with attention** (attention values can be calculated simultaneously for different parts of the sequence data). Outputs can also be computed in parallel (instead of a series like in RNN)\n",
        "- **transformers** can capture long-range contexts and dependencies in the data better than RNNs or CNNs; thus, longer connections can be learned.\n",
        "- **attention** allows each sequence position to have access to the entire input at each layer, while in RNNs and CNNs, the information needs to pass through many sequential processing steps, which makes it harder to learn\n",
        "- transformers make **no assumptions about the temporal/spatial relationships** across the data"
      ],
      "metadata": {
        "id": "-afE4kFxxzAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The first step is **input embedding**, which in the case of NLP is a **word embedding layer**: each word in the input (dictionary) maps to a vector of continuous values.\n",
        "\n",
        "The second step is **positional encoding**: RNN explicitely model the order in sequence data by processing the input elements sequentially, transformers don't (attention in stead). Therefore, the position in the sequence is introduced in the model using the `sine` and `cosine` functions (even and odd position indices).\n",
        "\n",
        "Now we have the encoder: first, the **multi-head attention** module, where the network learns to associate words in the input text (e.g. \"weather\" with \"sunny\"). It is called multi-head because this association is done (in parallel) for all (most) words in the input data."
      ],
      "metadata": {
        "id": "Rbnj2RqRp8jw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention <a name=\"attention-concept\"></a>\n",
        "\n",
        "Attention in ML is the ability of the model to automatically, dynamically and independently **highlight** and **use** the **salient parts of the input data**:\n",
        "\n",
        "- \"reading\" the raw data (e.g. words in a text) and converting these into distributed representations (one feature vector associated with each word position) $\\rightarrow$ This is the **encoder**!\n",
        "- list of feature vectors storing the output of the \"reader\" part of the model: sort of \"memory\" that can be retrieved later, not necessarily in the same order, without having to check/inspect all of them $\\rightarrow$ (some) encoded feature vectors + previous decoder's hidden states = on what to focus to produce the output\n",
        "- \"exploits\" the content of the memory to sequentially perform a task, at each time step having the ability to focus (bestow attention) on the content of one memory element (or a few, with different weights) $→$ at each time step, the score values (encoded feature vectors + decoder's hidden states) are normalized (softmax) $\\rightarrow$ **weights**: the encoded vectors are then scaled by the weights $\\rightarrow$ **context vector**, which is then fed into the decoder to generate the final output\n"
      ],
      "metadata": {
        "id": "dpU_2j2LT0HX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Alignment scores**: the encoded feature vectors ($\\text{h}_i$) and the previous decoder hidden states ($\\text{s}_{t-1}$) are used to compute a score (match between the input sequence and the current output at each position $t$), implemented with a feed-forward neural network:\n",
        "  - $\\text{e}_{t,i} = \\text{a}(\\text{s}_{t-1}, \\text{h}_i)$\n",
        "\n",
        "2. **Weights**: obtained by applying a softmax function to the alignment scores:\n",
        "  - $\\alpha_{t,i} = \\text{softmax}(\\text{e}_{t,i})$\n",
        "\n",
        "3. **Context vector**: fed to the decoder at each time step $t$, and computed as weighted sum of the encoder feature vectors:\n",
        "  - $\\text{c}_t = \\sum_{i=1}^T \\alpha_{t,i} \\text{h}_i$"
      ],
      "metadata": {
        "id": "pc6RwQ_-oygD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Attention mechanism <a name=\"attention-mechanism\"></a>"
      ],
      "metadata": {
        "id": "V7xrtiu3sBI-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input** + **query**:\n",
        "\n",
        "- e.g. input = image; query = 'what's the weather like?'\n",
        "- the network will probably have to <u>attend</u> (focuses on) the sky, first\n",
        "- this translates to copmuting the **similarity** between the **input matrix** and the **query vector** $\\rightarrow$ **similarity score**\n",
        "\n",
        "The triplet is: i) **query** (Q), ii) **key** (K), iii) **value** (V): queries are executed against key-value pairs\n",
        "\n",
        "- Q: $\\text{s}_{t-1}$: previous decoder's hidden states\n",
        "- K: $\\text{h}_i$: encoded feature vectors\n",
        "- V: $\\text{h}_i$: encoded feature values\n",
        "\n",
        "Similarly to the basic attention concept (above): i) Q and K are multiplied together (dot product) to produce a **matrix of scores** which reflects how much attention to pay on each query-key pair;\n",
        "ii) the matrix of scores is then normalised (`softmax function`) to produce the **attention weights**;\n",
        "iii) attention weights are then multiplied by the values (V) to obtain the output.\n",
        "\n",
        "1. each **query vector** $\\text{q} = \\text{s}_{t-1}$ is matched against keys (K: key vectors $\\text{k}_i$) to compute a **score value** (dot/scalar product):\n",
        "  - $\\text{e}_{q,k_i} = \\text{q} \\cdot \\text{k}_i$\n",
        "\n",
        "2. scores are normalised (softmax) to generate **weights**:\n",
        "  - $\\alpha_{q,k_i} = \\text{softmax}(\\text{e}_{q,k_i})$\n",
        "\n",
        "3. generalized attention is then computed as weighted sum of the value vectors (fetched from corresponding keys):\n",
        "  - $\\text{attention}(q,K,V) = \\sum_i{\\alpha_{q,k_i} \\text{v}_{k_i}}$"
      ],
      "metadata": {
        "id": "G-ErCVPCoxGf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Numerical example\n",
        "\n",
        "([adapted from here](https://machinelearningmastery.com/the-attention-mechanism-from-scratch/))\n",
        "\n",
        "Let's start with the input embeddings: actually, these would be generated by and encoder (embedding layer); here we declare the embeddings manually for four example words"
      ],
      "metadata": {
        "id": "P8r7wIjqwGnb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import scipy as sp"
      ],
      "metadata": {
        "id": "eJaX_VozxEUn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encoder representations of four different words\n",
        "word_1 = np.array([1, 0, 0])\n",
        "word_2 = np.array([0, 1, 0])\n",
        "word_3 = np.array([1, 1, 0])\n",
        "word_4 = np.array([0, 1, 1])"
      ],
      "metadata": {
        "id": "0hElJoc4wJOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then we generate weights to be then multiplied to the word embeddings to generate the queries, keys, and values (in actual practice, these weights would are learned during training $→$ **learning the weights** is always the job of the network!)"
      ],
      "metadata": {
        "id": "2HC6ay1Owuec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the weight matrices\n",
        "np.random.seed(42) # to allow us to reproduce the same attention values\n",
        "W_Q = np.random.randint(3, size=(3, 3))\n",
        "W_K = np.random.randint(3, size=(3, 3))\n",
        "W_V = np.random.randint(3, size=(3, 3))"
      ],
      "metadata": {
        "id": "t5M3COGFwc4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generating the queries (decoder hidden states at t-1), keys (encoded feature vectors) and values (encoded feature values)\n",
        "query_1 = word_1 @ W_Q\n",
        "key_1 = word_1 @ W_K\n",
        "value_1 = word_1 @ W_V\n",
        "\n",
        "query_2 = word_2 @ W_Q\n",
        "key_2 = word_2 @ W_K\n",
        "value_2 = word_2 @ W_V\n",
        "\n",
        "query_3 = word_3 @ W_Q\n",
        "key_3 = word_3 @ W_K\n",
        "value_3 = word_3 @ W_V\n",
        "\n",
        "query_4 = word_4 @ W_Q\n",
        "key_4 = word_4 @ W_K\n",
        "value_4 = word_4 @ W_V"
      ],
      "metadata": {
        "id": "e-FncUZ3w7mJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(word_1)\n",
        "print(query_1)\n",
        "print(key_1)\n",
        "print(value_1)"
      ],
      "metadata": {
        "id": "jWNV7arYxYXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have all the ingredients to calculate the alignment scores at time step $t$ for each input word (embedded):"
      ],
      "metadata": {
        "id": "4WKyrKa81oLS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from numpy import dot\n",
        "\n",
        "# scoring the first query vector against all key vectors\n",
        "scores = np.array([dot(query_1, key_1), dot(query_1, key_2), dot(query_1, key_3), dot(query_1, key_4)])"
      ],
      "metadata": {
        "id": "fWhYASqe1dZQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now **normalise the score values**: i) divide the score values by the square root of the dimensionality of the key vectors (in this case, three); ii) softmax.\n",
        "\n",
        "In this way, the **weights** are obtained:"
      ],
      "metadata": {
        "id": "YPu7spRF5Wm8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.special import softmax\n",
        "\n",
        "## computing the weights by a softmax operation\n",
        "weights = softmax(scores / key_1.shape[0] ** 0.5)"
      ],
      "metadata": {
        "id": "0638Lrf45pop"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(weights)"
      ],
      "metadata": {
        "id": "f0ip0wqL6aUy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Finally, the **attention output** is calculated as weighted sum of all four value vectors:"
      ],
      "metadata": {
        "id": "e-IPO2066mw5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# computing the attention by a weighted sum of the value vectors\n",
        "attention = (weights[0] * value_1) + (weights[1] * value_2) + (weights[2] * value_3) + (weights[3] * value_4)\n",
        "\n",
        "print(attention)"
      ],
      "metadata": {
        "id": "Hcic1rQl6f_B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In a more compact matrix representation we can code this as:"
      ],
      "metadata": {
        "id": "gD2hDVhm82WB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# stacking the word embeddings into a single array\n",
        "words = np.array([word_1, word_2, word_3, word_4])\n",
        "\n",
        "# generating the weight matrices\n",
        "np.random.seed(42)\n",
        "W_Q = np.random.randint(3, size=(3, 3))\n",
        "W_K = np.random.randint(3, size=(3, 3))\n",
        "W_V = np.random.randint(3, size=(3, 3))\n",
        "\n",
        "# generating the queries, keys and values\n",
        "Q = words @ W_Q\n",
        "K = words @ W_K\n",
        "V = words @ W_V\n",
        "\n",
        "# scoring the query vectors against all key vectors\n",
        "scores = Q @ K.transpose()\n",
        "\n",
        "# computing the weights by a softmax operation\n",
        "weights = softmax(scores / K.shape[1] ** 0.5, axis=1)\n",
        "\n",
        "# computing the attention by a weighted sum of the value vectors\n",
        "attention = weights @ V\n",
        "\n",
        "print(attention)"
      ],
      "metadata": {
        "id": "41805DG87QCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have attention values for each word (input element in the sequence) and for each embedded feature (3 in this toy example)"
      ],
      "metadata": {
        "id": "g0GEv-_E8stM"
      }
    }
  ]
}