{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "16f2f29e",
      "metadata": {
        "id": "16f2f29e"
      },
      "source": [
        "# ROC curves, AUC (area under the curve) and MCC (Matthews' correlation coefficient)\n",
        "\n",
        "In this notebook, we illustrate how to:\n",
        "\n",
        "1. draw the ROC curve\n",
        "2. calculate the AUC (area under the curve)\n",
        "3. calculate the MCC (Matthew's correlation coefficient)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading libraries\n",
        "\n",
        "First of all, we load some necessary general libraries:"
      ],
      "metadata": {
        "id": "5Z63YlH0OgfH"
      },
      "id": "5Z63YlH0OgfH"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e86c1bf",
      "metadata": {
        "id": "9e86c1bf"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data\n",
        "\n",
        "We are using results from a mock binary classification problem.\n",
        "This was based on the [breast cancer Wisconsin dataset](https://github.com/scikit-learn/scikit-learn/blob/6e9039160f0dfc3153643143af4cfdca941d2045/sklearn/datasets/data/breast_cancer.csv) from the Python library `sklearn`.\n",
        "\n",
        "In this dataset, the objective is to diagnose the status of breast cancer:\n",
        "\n",
        "- `0`: malignant cancer\n",
        "- `1`: benign cancer\n",
        "\n",
        "The dataset contains **569 examples**:\n",
        "\n",
        "- 212 malignant\n",
        "- 357 benign\n",
        "\n",
        "And the prediction (classification) is based on 30 numeric features related to the cancer lesions (size, shape etc.: full description can be found [here](https://scikit-learn.org/1.5/datasets/toy_dataset.html#breast-cancer-dataset))."
      ],
      "metadata": {
        "id": "zZ3vzxq4B0IQ"
      },
      "id": "zZ3vzxq4B0IQ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actually, in this example we used a random subset of the 30 features, by selecting 8 features: in this way, the problem was harder, and we obtained more classification errors, which is instrumental to the illustration of different metrics to measure model performance."
      ],
      "metadata": {
        "id": "m1Psru1cCvZR"
      },
      "id": "m1Psru1cCvZR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is imbalanced: the ratio between the two classes is not 1, but 0.6.\n",
        "Again, this is instrumental in showing the relative advantage of using different performance metrics rather than just looking at the error rate / overall accuracy."
      ],
      "metadata": {
        "id": "nSpEUJkJDJVV"
      },
      "id": "nSpEUJkJDJVV"
    },
    {
      "cell_type": "markdown",
      "source": [
        "An 80% / 20% training / test data split was used to train the classification model and measure performance: the **test results** are used here to show ROC curves, AUC and MCC."
      ],
      "metadata": {
        "id": "L8aDDsZnKj2d"
      },
      "id": "L8aDDsZnKj2d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We trained two classification models: the second (`mod2`) was designed as to increase overfitting and produce results which are biased towards the majority class (useful for the illustration of performance metrics)."
      ],
      "metadata": {
        "id": "GzIu-gF7ZxK9"
      },
      "id": "GzIu-gF7ZxK9"
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_URL = 'https://raw.githubusercontent.com/ne1s0n/bioinformateachers/refs/heads/main/dlb/data/predictions.csv'"
      ],
      "metadata": {
        "id": "4aQPSBr4VSAZ"
      },
      "id": "4aQPSBr4VSAZ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataframe contains:\n",
        "\n",
        "- the **original test observation** (the \"truth\": malignant or benign)\n",
        "- the **predicted class** (binary) for the base and alternative (mod2) models\n",
        "- the **two probabilities**: of being '0' (malignant) or '1' (benign), for the base and alternative models (mod2)"
      ],
      "metadata": {
        "id": "5Y_ioaAiDls6"
      },
      "id": "5Y_ioaAiDls6"
    },
    {
      "cell_type": "code",
      "source": [
        "bc_data = pd.read_csv(DATASET_URL)\n",
        "bc_data.head()"
      ],
      "metadata": {
        "id": "tTBPkZukVMax"
      },
      "id": "tTBPkZukVMax",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The test dataset was generated by taking a 20% random subset of the data: **114 test examples**."
      ],
      "metadata": {
        "id": "Jon0w7xTEDJu"
      },
      "id": "Jon0w7xTEDJu"
    },
    {
      "cell_type": "code",
      "source": [
        "len(bc_data)"
      ],
      "metadata": {
        "id": "gTG2bDkmpiEC"
      },
      "id": "gTG2bDkmpiEC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 1: benign\n",
        "## 0: malignant\n",
        "bc_data[['y_test']].value_counts()"
      ],
      "metadata": {
        "id": "JsDH2sLXly9z"
      },
      "id": "JsDH2sLXly9z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Let's get a look at the original **confusion matrix**: first we get the two vectors of predictions and observations, and then construct the matrix of correct predictions (diagonal) and errors (off diagonal).\n",
        "\n",
        "(Remember: in the `sklearn` confusion matrix, true labels are on the rows, predicted labels are on the columns)"
      ],
      "metadata": {
        "id": "HG8w1lIZEZ1O"
      },
      "id": "HG8w1lIZEZ1O"
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.array(bc_data['y_test'])\n",
        "y_pred = np.array(bc_data['y_pred'])"
      ],
      "metadata": {
        "id": "kLVKr2XQpRGG"
      },
      "id": "kLVKr2XQpRGG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import the metrics class\n",
        "from sklearn import metrics\n",
        "\n",
        "cnf_matrix = metrics.confusion_matrix(y_test, y_pred)\n",
        "cnf_matrix"
      ],
      "metadata": {
        "id": "emzEiXH1ypKj"
      },
      "id": "emzEiXH1ypKj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "The next step will be to bring the **probabilities of prediction** in the game, not just the predicted classes.\n",
        "\n",
        "Keep in mind that the predicted classes have been obtained based on the **0.5 threshold**: each test example is attributed to the class whose probability is larger than 50%."
      ],
      "metadata": {
        "id": "A45ZgELXE9nP"
      },
      "id": "A45ZgELXE9nP"
    },
    {
      "cell_type": "code",
      "source": [
        "y_probs = np.array(bc_data[bc_data.columns[3:5]])\n",
        "print(y_probs[0:5,:])"
      ],
      "metadata": {
        "id": "iqwexZ890Pwj"
      },
      "id": "iqwexZ890Pwj",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "### ROC curves\n",
        "\n",
        "This is a binary classification problem, therefore the model usually focuses on the probability for just one class (being the other unambiguously obtained as reciprocal to 1).\n",
        "Most commonly, the probability of class `1` (\"case\") is modeled:\n",
        "\n",
        "$$\n",
        "P(y=1 | X)\n",
        "$$\n",
        "\n",
        "Therefore, this probability is used in the calculation of the **ROC curve**."
      ],
      "metadata": {
        "id": "3wIpzF82Fh1K"
      },
      "id": "3wIpzF82Fh1K"
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_curve, auc, roc_auc_score"
      ],
      "metadata": {
        "id": "FZpFdT062M10"
      },
      "id": "FZpFdT062M10",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prob_y_eq_1 = y_probs[:,1]"
      ],
      "metadata": {
        "id": "XEqQFSZ53M_k"
      },
      "id": "XEqQFSZ53M_k",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The **ROC curve** is based on looking at classification results from the perspective of all (many) classification thresholds, not just the standard 50%.\n",
        "\n",
        "This means that probabilities ($P(y=1)$) are evaluated against threshold 0%, 0.5% 10% $\\ldots$ 90%, 95%, 100%): for each threshold the **false positive rate** (FP/(FP+TN)) and the **true positive rate** (TP/(TP+FN) = 1-FNR) are calculated and then plotted against each other."
      ],
      "metadata": {
        "id": "iNXRSCLcMrIB"
      },
      "id": "iNXRSCLcMrIB"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The function `roc_curve` from `sklearn` takes in input the correct test labels (the \"truth\") and the prediction probabilities obtained from the classification model.\n",
        "\n",
        "This function returns the ingredients needed to draw the ROC curve: the FPR and TPR calculated against several classification thresholds (by default, 20 thresholds are considered):"
      ],
      "metadata": {
        "id": "-zLjFGkuO9dl"
      },
      "id": "-zLjFGkuO9dl"
    },
    {
      "cell_type": "code",
      "source": [
        "fpr, tpr, thrs = roc_curve(y_test, prob_y_eq_1)"
      ],
      "metadata": {
        "id": "0p20Ypqm2-97"
      },
      "id": "0p20Ypqm2-97",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We obtain **20 values** for **FPR** and for **TPR**:"
      ],
      "metadata": {
        "id": "fsH9-JKEPgS_"
      },
      "id": "fsH9-JKEPgS_"
    },
    {
      "cell_type": "code",
      "source": [
        "len(fpr)"
      ],
      "metadata": {
        "id": "AadGlZohIz-a"
      },
      "id": "AadGlZohIz-a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(np.vstack((fpr,tpr)).T, columns=['FPR','TPR'])\n",
        "df.head(10)"
      ],
      "metadata": {
        "id": "BpPpbOiQQcTp"
      },
      "id": "BpPpbOiQQcTp",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now have all the elements to plot the ROC curve for this classification problem.\n",
        "\n",
        "Typically, a ROC curve is visually contrasted against chance accuracy, which for a binary classification problem is 50%, and it is represented by a straight line bisecting the plot:"
      ],
      "metadata": {
        "id": "YmKiyMJMRRGr"
      },
      "id": "YmKiyMJMRRGr"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(\n",
        "    fpr,\n",
        "    tpr,\n",
        "    color=\"darkorange\",\n",
        "    lw=lw,\n",
        "    #label=\"ROC curve (area = %0.2f)\" % roc_auc,\n",
        ")\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver operating characteristic example\")\n",
        "#plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "GiUzV-Bpwget"
      },
      "id": "GiUzV-Bpwget",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ideally, the ROC curve of the chose classification model should be as close as possible to the **top left corner** of the above plot; conversely, the closer the ROC curve gets to the dashed line (chance accuracy), the worse the performance of the model."
      ],
      "metadata": {
        "id": "zCAg_0O4MFnq"
      },
      "id": "zCAg_0O4MFnq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### AUC\n",
        "\n",
        "The area under the (ROC) curve -the **AUC**- provides a summary numeric score for the model performance, in terms of **True** and **False Positive Rates**, over **multiple classification thresholds**.\n",
        "\n",
        "This is a very effective way of summarising the information visually conveyed by the plot of the ROC curve:\n",
        "\n",
        "- if the ROC curve follows exactly the left top corner of the plot, we have perfect classification accuracy, and the AUC is 100% (**AUC = 1** : the entire plotting area is under the ROC curve)\n",
        "- if the ROC curve is collapsed with the diagonal bisecting the plot, we have perfect chance accuracy, and the AUC is 50% (**AUC = 0.5**); our classification modes is no better than tossing a coin in making predictions!\n",
        "\n",
        "Intuitively, the higher the AUC the better the binary classification model.\n",
        "However, what is to be considered a good AUC score obvioulsy depends a lot on the specific classification problem at hand.\n",
        "Generally speaking, **AUC > 0.8** usually indicates a good model performance."
      ],
      "metadata": {
        "id": "t_r1c1p6RB3U"
      },
      "id": "t_r1c1p6RB3U"
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc = auc(fpr, tpr)"
      ],
      "metadata": {
        "id": "k-73JAZu4S2o"
      },
      "id": "k-73JAZu4S2o",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"AUC is\", round(roc_auc,3))"
      ],
      "metadata": {
        "id": "9x0as1a54esr"
      },
      "id": "9x0as1a54esr",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can easily add the AUC value to the ROC curve plot:"
      ],
      "metadata": {
        "id": "H5cVrR3tl5Y7"
      },
      "id": "H5cVrR3tl5Y7"
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(\n",
        "    fpr,\n",
        "    tpr,\n",
        "    color=\"darkorange\",\n",
        "    lw=lw,\n",
        "    label=\"ROC curve (area = %0.3f)\" % roc_auc\n",
        ")\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver operating characteristic example\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "7T1bLrQgRKvN"
      },
      "id": "7T1bLrQgRKvN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see below, the overall accuracy of classification (1 - error rate) is $0.904$, but with a sizeable difference between the two classes: the accuracy among positive cases (**TPR**) is $0.945$, and it is higher than the accuracy in the negative cases (**TNR**: $0.829$).\n",
        "\n",
        "This is most likely linked to the class imbalance: in this dataset, the number of positive cases (benign breast cancer) is 60% larger than the number of negative cases (malignant brest cancer).\n",
        "\n",
        "The AUC combines all this information into a single metric, by looking at both TPR and FPR (1- FNR) over all possible classification thresholds: the AUC depends both on TPR / FPR and on the probabilities of classification."
      ],
      "metadata": {
        "id": "pe8ZNpcqippA"
      },
      "id": "pe8ZNpcqippA"
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = (y_test == y_pred).sum()/len(y_test)\n",
        "print(round(accuracy, 3))"
      ],
      "metadata": {
        "id": "Us6EGulTe_r5"
      },
      "id": "Us6EGulTe_r5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_positives = len(bc_data[bc_data[\"y_test\"]==1])\n",
        "n_negatives = len(bc_data[bc_data[\"y_test\"]==0])\n",
        "\n",
        "print(\"N. of positive test examples:\", n_positives)\n",
        "print(\"N. of negative test examples:\", n_negatives)"
      ],
      "metadata": {
        "id": "TNTnwpgpfzWC"
      },
      "id": "TNTnwpgpfzWC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "true_positive_preds = len(bc_data[(bc_data.y_test==1) & (bc_data.y_pred == 1)])\n",
        "true_negative_preds = len(bc_data[(bc_data.y_test==0) & (bc_data.y_pred == 0)])\n",
        "\n",
        "tpr_val = true_positive_preds/n_positives\n",
        "tnr_val = true_negative_preds/n_negatives\n",
        "\n",
        "print(\"TPR is:\", round(tpr_val,3))\n",
        "print(\"TNR is:\", round(tnr_val,3))"
      ],
      "metadata": {
        "id": "nBRv3_ORgGj-"
      },
      "id": "nBRv3_ORgGj-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Alternative model (with overfitting)\n",
        "\n",
        "We classified the same data (breast cancer) usign a second alternative model, where we did some overfitting on purpose."
      ],
      "metadata": {
        "id": "N8Dj9MTeamT3"
      },
      "id": "N8Dj9MTeamT3"
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred2 = np.array(bc_data['y_pred_mod2'])\n",
        "\n",
        "### confusion matrix from model 2\n",
        "cnf_matrix2 = metrics.confusion_matrix(y_test, y_pred2)\n",
        "cnf_matrix2"
      ],
      "metadata": {
        "id": "0zKAxBjXeNV9"
      },
      "id": "0zKAxBjXeNV9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This second model made the same number of errors on the test data (n_error = 11), and had therefore the exact same overall accuracy: $0.904$ (see below).\n",
        "\n",
        "However, the distribution of errors is now different:\n",
        "\n",
        "- TPR = 72/73 = $0.986$\n",
        "- TNR = 31/41 = $0.756$\n",
        "\n",
        "We see that the model is now more skewed towards the majority class, thus making many more errors in the minority class.\n",
        "The overall accuracy is not able to capture this difference between the two classification models, therefore we can resort to the analysis of ROC curves.\n",
        "\n",
        "We see (below) that the ROC curve for model 2 (green dotted line) appears to be more often than not closer to chance accuracy compared to base model (solid yellow line): there is therefore [second-order stochastic dominance](https://en.wikipedia.org/wiki/Stochastic_dominance) between the two models.\n",
        "\n",
        "This is confirmed by the AUC: AUC for model 2 is 0.949, which is lower than that for the first model, which is 0.956 (not much, but still lower, hence better TPR / TNR trade-off over all classification thresholds)."
      ],
      "metadata": {
        "id": "salwwGGWwOdG"
      },
      "id": "salwwGGWwOdG"
    },
    {
      "cell_type": "code",
      "source": [
        "### overall accuracy from model 2\n",
        "accuracy = (y_test == y_pred2).sum()/len(y_test)\n",
        "print(round(accuracy, 3))"
      ],
      "metadata": {
        "id": "LINGRs0jb4ND"
      },
      "id": "LINGRs0jb4ND",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### TPR and TNR from model 2\n",
        "true_positive_preds = len(bc_data[(bc_data.y_test==1) & (bc_data.y_pred_mod2 == 1)])\n",
        "true_negative_preds = len(bc_data[(bc_data.y_test==0) & (bc_data.y_pred_mod2 == 0)])\n",
        "\n",
        "tpr_mod2 = true_positive_preds/n_positives\n",
        "tnr_mod2 = true_negative_preds/n_negatives\n",
        "\n",
        "print(\"TPR is:\", round(tpr_mod2,3))\n",
        "print(\"TNR is:\", round(tnr_mod2,3))"
      ],
      "metadata": {
        "id": "EqSpN9IwcN6Q"
      },
      "id": "EqSpN9IwcN6Q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ROC curve and AUC from the alternative classification model"
      ],
      "metadata": {
        "id": "pfmVNG2OuOfi"
      },
      "id": "pfmVNG2OuOfi"
    },
    {
      "cell_type": "code",
      "source": [
        "y_probs = np.array(bc_data[bc_data.columns[5:7]])\n",
        "prob_y_eq_1_mod2 = y_probs[:,1]\n",
        "fpr2, tpr2, thrs = roc_curve(y_test, prob_y_eq_1_mod2)"
      ],
      "metadata": {
        "id": "PHSYKVhwaqtA"
      },
      "id": "PHSYKVhwaqtA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "roc_auc2 = auc(fpr2, tpr2)\n",
        "print(\"AUC for the alternative model is\", round(roc_auc2,3))"
      ],
      "metadata": {
        "id": "pXzvalzQt4lx"
      },
      "id": "pXzvalzQt4lx",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure()\n",
        "lw = 2\n",
        "plt.plot(\n",
        "    fpr,\n",
        "    tpr,\n",
        "    color=\"darkorange\",\n",
        "    lw=lw,\n",
        "    label=\"ROC curve (area = %0.3f)\" % roc_auc,\n",
        ")\n",
        "plt.plot(\n",
        "    fpr2,\n",
        "    tpr2,\n",
        "    color=\"darkgreen\",\n",
        "    lw=lw,\n",
        "    linestyle=\":\",\n",
        "    label=\"ROC curve (area = %0.3f)\" % roc_auc2,\n",
        ")\n",
        "plt.plot([0, 1], [0, 1], color=\"navy\", lw=lw, linestyle=\"--\")\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.title(\"Receiver operating characteristic example\")\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "f7u_1EkvbHve"
      },
      "id": "f7u_1EkvbHve",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### MCC"
      ],
      "metadata": {
        "id": "Iaw60Y7QRLzb"
      },
      "id": "Iaw60Y7QRLzb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The ROC AUC is an excellent metric for binary classification problems, that summarizes the accuracy in both classes and makes a trade-off between TPR and TNR (there are also extensions to multiclass classification: [volume under the ROC surface](https://link.springer.com/chapter/10.1007/978-3-540-39857-8_12)). Under some extreme circumstances, though, AUC can be a misleading measure of the model performance.\n",
        "\n",
        "The AUC only takes into account two of the four basic ratios from the confusion matrix: TPR and FPR = 1-TNR (the accuracy measured on the true labels). However, when data are strongly imbalanced this can be suboptimal:\n",
        "- e.g. positive examples >> negative examples: > false negatives (positive examples wrongly classified as negative)\n",
        "- $\\rightarrow$ small change in  FNR (TPR = 1 - FNR) (the denominator is the sum of the many true positives and the few false negatives)\n",
        "- $\\rightarrow$ larger change in NPV (calculations restricted to the few negative predictions)\n",
        "\n",
        "For example, if we have 1020 'cases' (positive examples) and make twelve errors (FN = 12), the TPR would be 1008/1020 = 98.8% (calculations within positive cases only).\n",
        "If FN increase by 50% (new FN = 18), the TPR would decrease to 98.2% (almost imperceptible change).\n",
        "The FPR would remain unchanged (only the number of FN has increased), thereby producing almost no change in ROC-AUC.\n",
        "\n",
        "However, with the same results and, say, 20 true negatives (remember,\n",
        "this is the highly unbalanced minority class), the NPV would go from\n",
        "62.5% to 52.6%: this sharp decrease would hardly go unnoticed!\n",
        "\n",
        "Instead of considering only two error metrics (the accuracy\n",
        "from the perspective of the true labels), a better option would be to\n",
        "consider all four: TPR, TNR, PPV and NPV. This is the **Matthews\n",
        "Correlation Coefficient** (**MCC**), and offers another perspective to model performance:"
      ],
      "metadata": {
        "id": "F5RlOn3bqD9G"
      },
      "id": "F5RlOn3bqD9G"
    },
    {
      "cell_type": "markdown",
      "source": [
        "$$\n",
        "\\phi = \\frac{(TP \\cdot TN - FP \\cdot FN)}{\\sqrt{(TP+FP) \\cdot (TP+FN) \\cdot (TN+FP) \\cdot (TN+FN)}}\n",
        "$$"
      ],
      "metadata": {
        "id": "Nz8nRg1_HyjY"
      },
      "id": "Nz8nRg1_HyjY"
    },
    {
      "cell_type": "code",
      "source": [
        "## get the prediction / error counts\n",
        "tn, fp, fn, tp = cnf_matrix.ravel()"
      ],
      "metadata": {
        "id": "_8DY9KLggqn3"
      },
      "id": "_8DY9KLggqn3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## calculate MCC (a.k.a. phi) by hand\n",
        "phi =(tp*tn - fp*fn) / np.sqrt((tp+fp)*(fn+tn)*(tp+fn)*(fp+tn))\n",
        "print(\"MCC is\", round(phi,3))"
      ],
      "metadata": {
        "id": "gAB1pjsjg77M"
      },
      "id": "gAB1pjsjg77M",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## use the function matthews_corrcoef from scikit-learn\n",
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "y_test = np.array(bc_data['y_test'])\n",
        "y_pred = np.array(bc_data['y_pred'])\n",
        "matthews_corrcoef(y_test, y_pred)"
      ],
      "metadata": {
        "id": "6Rn8zQfafHcz"
      },
      "id": "6Rn8zQfafHcz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "What with the second classification model? (column '`y_pred_mod2`') in the dataset:"
      ],
      "metadata": {
        "id": "uZSjlRPs_VR7"
      },
      "id": "uZSjlRPs_VR7"
    },
    {
      "cell_type": "code",
      "source": [
        "y_test = np.array(bc_data['y_test'])\n",
        "y_pred = np.array(bc_data['y_pred_mod2'])"
      ],
      "metadata": {
        "id": "otadIBGIiCcQ"
      },
      "id": "otadIBGIiCcQ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phi2 = matthews_corrcoef(y_test, y_pred)\n",
        "print(\"MCC for model 2 is\", round(phi2,3))"
      ],
      "metadata": {
        "id": "WsojLmMxiES5"
      },
      "id": "WsojLmMxiES5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MCC for model 2 is higher than that of model 1, although results in terms of AUC are the other way around.\n",
        "Let's look at the distribution of predictions and errors for the two models, to try and understand what is going on.\n",
        "\n",
        "Recap: this is the structure of the the confusion matrix:\n",
        "\n",
        "```\n",
        "\n",
        "      | pred - | pred +\n",
        "------|--------|--------\n",
        "obs - |   TN   |   FP\n",
        "obs + |   FN   |   TP\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "tU_zM_tQAArK"
      },
      "id": "tU_zM_tQAArK"
    },
    {
      "cell_type": "markdown",
      "source": [
        "- more positive than negative observations: 73 vs 41\n",
        "- model 2 gives more FP and fewer FN\n",
        "\n",
        "<u>Observation-wise</u>\n",
        "- we expect TPR to not change much (TP are most abundant)\n",
        "- we expect FPR to be higher for model 2 (more errors in the minority class)\n",
        "- therefore AUC will be lower for model 2 (similar TPR, larger FPR)\n",
        "\n",
        "<u>Prediction-wise</u>\n",
        "- we expect PPV to not change much (again, TP are most abundant)\n",
        "- we expect NPV to be higher for model 2 (only 1 error among negative predictions)"
      ],
      "metadata": {
        "id": "0ip5zvWkAt18"
      },
      "id": "0ip5zvWkAt18"
    },
    {
      "cell_type": "code",
      "source": [
        "np.concatenate((cnf_matrix, cnf_matrix2))"
      ],
      "metadata": {
        "id": "zdvdNIw2_nz5"
      },
      "id": "zdvdNIw2_nz5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model 1\n",
        "\n",
        "tn, fp, fn, tp = cnf_matrix.ravel()\n",
        "\n",
        "FDR = fp/(fp+tp)\n",
        "FOR = fn/(tn+fn)\n",
        "PPV = tp/(tp+fp)\n",
        "NPV = tn/(tn+fn)\n",
        "\n",
        "FPR = fp/(fp+tn)\n",
        "FNR = fn/(tp+fn)\n",
        "TPR = tp/(tp+fn)\n",
        "TNR = tn/(fp+tn)\n",
        "\n",
        "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "ndec = 3\n",
        "\n",
        "dict1 = {'accuracy' : round(accuracy, ndec), 'AUC' : round(roc_auc,ndec), 'TPR' : round(TPR,ndec),\n",
        "         'TNR' : round(TNR,ndec), 'FPR' : round(FPR,ndec), 'FNR' : round(FNR,ndec),\n",
        "         'FDR' : round(FDR,ndec), 'FOR' : round(FOR,ndec), 'PPV' : round(PPV,ndec),\n",
        "         'NPV' : round(NPV,ndec), 'MCC' : round(phi,ndec)}"
      ],
      "metadata": {
        "id": "ds8AJJHkiF95"
      },
      "id": "ds8AJJHkiF95",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## model 2\n",
        "\n",
        "tn, fp, fn, tp = cnf_matrix2.ravel()\n",
        "\n",
        "FDR = fp/(fp+tp)\n",
        "FOR = fn/(tn+fn)\n",
        "PPV = tp/(tp+fp)\n",
        "NPV = tn/(tn+fn)\n",
        "\n",
        "FPR = fp/(fp+tn)\n",
        "FNR = fn/(tp+fn)\n",
        "TPR = tp/(tp+fn)\n",
        "TNR = tn/(fp+tn)\n",
        "\n",
        "accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
        "ndec = 3\n",
        "\n",
        "dict2 = {'accuracy' : round(accuracy, ndec), 'AUC' : round(roc_auc2,ndec), 'TPR' : round(TPR,ndec),\n",
        "         'TNR' : round(TNR,ndec), 'FPR' : round(FPR,ndec), 'FNR' : round(FNR,ndec),\n",
        "         'FDR' : round(FDR,ndec), 'FOR' : round(FOR,ndec), 'PPV' : round(PPV,ndec),\n",
        "         'NPV' : round(NPV,ndec), 'MCC' : round(phi2,ndec)}"
      ],
      "metadata": {
        "id": "ufVaXp6YjPXl"
      },
      "id": "ufVaXp6YjPXl",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame.from_records([dict1, dict2],index=['model_1', 'model_2'])\n",
        "df"
      ],
      "metadata": {
        "id": "MaCwmXd5kSMX"
      },
      "id": "MaCwmXd5kSMX",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a matter of fact, we see:\n",
        "\n",
        "- for model 2, TPR is 4.3% higher, but FPR is 42.7% higher: these are the two ingredients of AUC, which is therefore lower for model 2 (worst relative performance)\n",
        "- for model 2, PPV is 3.3% lower, but NPV is 8.3% higher, and this is reflected in a slightly higher MCC (better relative model performance)\n"
      ],
      "metadata": {
        "id": "yszHcA0DHDpZ"
      },
      "id": "yszHcA0DHDpZ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}