{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Confusion matrix\n",
        "\n",
        "In this notebook, we illustrate how to:\n",
        "\n",
        "1. build the confusion matrix\n",
        "2. plot the confusion matrix\n",
        "3. extract the basic error rates from the confusion matrix"
      ],
      "metadata": {
        "id": "IM3kYD6kfSt8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the libraries"
      ],
      "metadata": {
        "id": "g-wJZSOqbPJ_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sq12QvKqfQKS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get the data\n",
        "\n",
        "We are using results from a mock binary classification problem.\n",
        "This was based on the [breast cancer Wisconsin dataset](https://github.com/scikit-learn/scikit-learn/blob/6e9039160f0dfc3153643143af4cfdca941d2045/sklearn/datasets/data/breast_cancer.csv) from the Python library `sklearn`.\n",
        "\n",
        "In this dataset, the objective is to diagnose the status of breast cancer:\n",
        "\n",
        "- `0`: malignant cancer\n",
        "- `1`: benign cancer\n",
        "\n",
        "The dataset contains **569 examples**:\n",
        "\n",
        "- 212 malignant\n",
        "- 357 benign\n",
        "\n",
        "And the prediction (classification) is based on 30 numeric features related to the cancer lesions (size, shape etc.: full description can be found [here](https://scikit-learn.org/1.5/datasets/toy_dataset.html#breast-cancer-dataset))"
      ],
      "metadata": {
        "id": "LKt3NNJ6bXiH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Actually, in this example we used a random subset of the 30 features, by selecting 8 features: in this way, the problem was harder, and we obtained more classification errors, which is instrumental to the illustration of different metrics to measure model performance."
      ],
      "metadata": {
        "id": "3VEszy-dbf7G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is imbalanced: the ratio between the two classes is not 1, but 0.6.\n",
        "Again, this is instrumental in showing the relative advantage of using different performance metrics rather than just looking at the error rate / overall accuracy.\n",
        "\n",
        "An 80% / 20% training / test data split was used to train the classification model and measure performance: the **test results** are used here to show ROC curves, AUC and MCC."
      ],
      "metadata": {
        "id": "MAXxhK1dnBcZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "From the **test dataset**, we read only the two columns that we need to for the confusion matrix:\n",
        "\n",
        "- the vector of **observed test labels**\n",
        "- the vector of **predicted labels** (classes)"
      ],
      "metadata": {
        "id": "inFg8m3fhnyG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DATASET_URL = 'https://raw.githubusercontent.com/ne1s0n/bioinformateachers/refs/heads/main/dlb/data/predictions.csv'"
      ],
      "metadata": {
        "id": "LWNyq_hWU7Xc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns = ['y_test', 'y_pred']\n",
        "\n",
        "bc_data = pd.read_csv(DATASET_URL, usecols=columns)\n",
        "bc_data.head()"
      ],
      "metadata": {
        "id": "llnG_U9TU9wj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = bc_data['y_pred']\n",
        "observations = bc_data['y_test']\n",
        "\n",
        "predicted_labels = np.where(predictions == 1.0, \"benign\", \"malignant\")\n",
        "target_labels = np.where(observations == 1.0, \"benign\", \"malignant\")"
      ],
      "metadata": {
        "id": "DpCktl3HXg-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can first have a look at the class distribution (malignant/benign) among observations and predictions:"
      ],
      "metadata": {
        "id": "mKQF_DnVpOX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import index\n",
        "\n",
        "labs, counts = np.unique(predicted_labels, return_counts=True)\n",
        "dict1 = {k:v for (k,v) in zip(labs,counts)}\n",
        "dict1['set'] = 'predictions'\n",
        "\n",
        "labs, counts = np.unique(target_labels, return_counts=True)\n",
        "dict2 = {k:v for (k,v) in zip(labs,counts)}\n",
        "dict2['set'] = 'observations'\n",
        "\n",
        "\n",
        "df = pd.DataFrame.from_records([dict1,dict2])\n",
        "df = df.set_index('set')\n",
        "df"
      ],
      "metadata": {
        "id": "U_QnsoxzRjIl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.plot.bar(rot=0)"
      ],
      "metadata": {
        "id": "7H5YP1DbiA2F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## The confusion matrix\n",
        "\n",
        "The confusion matrix has the following basic form; for a binary classification problem, it is a 2x2 table with -usually- observed classes on the rows and predicted classes on the columns:\n",
        "\n",
        "- TN: true negatives\n",
        "- FP: false positives\n",
        "- FN: false negatives\n",
        "- TP: true positives"
      ],
      "metadata": {
        "id": "QFmgVY5EpbTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```\n",
        "\n",
        "      | pred - | pred +\n",
        "------|--------|--------\n",
        "obs - |   TN   |   FP\n",
        "obs + |   FN   |   TP\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "Kt2nM805XwLx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use the  `confusion_matrix` function from `scikit-learn` to construct the confusion matrix from our vectors of observed and predicted labels (more [details here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)).\n",
        "By specifying the labels, you can change the order of positive and negative cases (1s and 0s) in the confusion matrix (be mindful of this!)."
      ],
      "metadata": {
        "id": "qYiro7s-pzZf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "conf_mat_df = confusion_matrix(y_true = target_labels, y_pred = predicted_labels, labels=[\"malignant\",\"benign\"])\n",
        "print(conf_mat_df)"
      ],
      "metadata": {
        "id": "qz1ziMsYQMNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "34/41"
      ],
      "metadata": {
        "id": "SAbV09rp9x1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bottom-right cell contains the number of true positives (TP): we can do a quick sanity check by subsetting from the dataset of test results only the rows where positive observations are predicted correctly: we get the n. 69, which is correct!"
      ],
      "metadata": {
        "id": "SjE-dozy6cCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## sanity check\n",
        "## let's get the n. of true positives (y_test == 1 AND y_pred == 1)\n",
        "bc_data.loc[(bc_data['y_test'] == 1) & (bc_data['y_pred'] == 1)].shape[0]"
      ],
      "metadata": {
        "id": "_jnVpffAYURa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The confusion matrix can be plotted by using a heatmap:"
      ],
      "metadata": {
        "id": "YuNWfpgo9gGt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sn\n",
        "\n",
        "figure = plt.figure(figsize=(8, 8))\n",
        "sn.heatmap(conf_mat_df, annot=True,cmap=plt.cm.Blues)\n",
        "plt.tight_layout()\n",
        "plt.ylabel('True label')\n",
        "plt.xlabel('Predicted label')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "5Azu-CTZnEVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "conf_mat_norm = confusion_matrix(target_labels, predicted_labels, normalize='true', labels=[\"malignant\",\"benign\"])\n",
        "print(conf_mat_norm)"
      ],
      "metadata": {
        "id": "TrhRmbtanYZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
      ],
      "metadata": {
        "id": "Y_nNPxmjPnit"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_mat_norm)"
      ],
      "metadata": {
        "id": "tejos4I7P8U6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "disp.plot(xticks_rotation=45)"
      ],
      "metadata": {
        "id": "ZlC0mkQ2QA9N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic error metrics from the confusion matrix"
      ],
      "metadata": {
        "id": "FKz9RVkB7JgO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The basic error metric is the overall **error rate**, which is the ratio between the n. of errors and the total n. of predictions:\n",
        "\n",
        "```\n",
        "                 # errors\n",
        "error rate = -------------------\n",
        "                # predictions\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "E147gS7SGWso"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The obvious counterpart is the overall **accuracy**:\n",
        "\n",
        "```\n",
        "                 # correct predictions\n",
        "accuracy = -----------------------------\n",
        "                   # predictions\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "whPOtkvWHILh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## ERROR RATE\n",
        "results = bc_data['y_test'] != bc_data['y_pred']\n",
        "error_rate = results.sum()/len(results)\n",
        "\n",
        "print(\"The error rate is:\", round(error_rate, 3), \"(or\", round(error_rate, 3)*100, \"%)\")"
      ],
      "metadata": {
        "id": "dGYlmFyEHf5M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## ACCURACY\n",
        "results = bc_data['y_test'] == bc_data['y_pred']\n",
        "accuracy = results.sum()/len(results)\n",
        "\n",
        "print(\"The overall accuracy is:\", round(accuracy, 3), \"(or\", round(accuracy, 3)*100, \"%)\")"
      ],
      "metadata": {
        "id": "YCWavNO0D6BQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can calculate these metrics by using the `accuracy_score()` function from `scikit-learn`:"
      ],
      "metadata": {
        "id": "RnfHOt2BIFXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "accuracy = accuracy_score(target_labels, predicted_labels)\n",
        "print(\"Accuracy is:\", accuracy)"
      ],
      "metadata": {
        "id": "m5Nk2WW0Zs6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Error rate is\", 1-accuracy)"
      ],
      "metadata": {
        "id": "AOQZoaY8adls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Error breakdown"
      ],
      "metadata": {
        "id": "JpCIzXETIaFo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tn, fp, fn, tp = conf_mat_df.ravel()\n",
        "\n",
        "print(\"TN;\", tn)\n",
        "print(\"FP:\", fp)\n",
        "print(\"FN:\", fn)\n",
        "print(\"TP:\", tp)"
      ],
      "metadata": {
        "id": "DQ256v8maayd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can look at results in the confusion matrix, either from the perspective of predictions (column-wise) or from that of the observed values (row-wise).\n",
        "\n",
        "From the perspective of the observed (true) values, we get the following basic error and accuracy metrics:\n",
        "\n",
        "- FPR: false positive rate\n",
        "- FNR: false negative rate\n",
        "- TNR: true negative rate\n",
        "- TPR: true positive rate\n",
        "\n",
        "```\n",
        "\n",
        "      | pred - | pred + |     error        |    accuracy\n",
        "------|--------|--------|------------------|-----------------\n",
        "obs - |   TN   |   FP   | FPR = FP/(TN+FP) | TNR = TN/(TN+FP)\n",
        "obs + |   FN   |   TP   | FNR = FN/(FN+TP) | TPR = TP/(FN+TP)\n",
        "\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "kMai4UPdAiMk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extension to the multiclass confusion matrix"
      ],
      "metadata": {
        "id": "TYAU3ybs7XDW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dtKl6zv77ajU"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}