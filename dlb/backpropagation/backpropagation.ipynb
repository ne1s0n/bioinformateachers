{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f30817",
   "metadata": {},
   "source": [
    "## Backpropagation example\n",
    "\n",
    "With this numerical example we illustrate how forward- and back-propagation work to learn the weights (parameters) of a **neural network**.\n",
    "\n",
    "We use a very simple network: this is far from a real-application network, but serves well the purpose of showing with numbers what happens in the network.\n",
    "\n",
    "In particular, the following simplifications apply:\n",
    "- only one training example (one record, with two features $x_1$ and $x_2$)\n",
    "- two numerical outputs $y = [y_1,y_2]$ (e.g. two continuous measurements on the same training example)\n",
    "- one hidden (intermediate) layer with two nodes\n",
    "- no bias terms\n",
    "\n",
    "<img src=\"simple_network.jpg\" alt=\"network\" style=\"width: 800px;\"/>\n",
    "\n",
    "We use the notation:\n",
    "- $z_{ij}$ to indicate the linear combination of inputs in layer $i$ and node $j$\n",
    "- $a_{ij}$ to indicate the activated linear combination $z$ in layer $i$ and node $j$\n",
    "- $\\hat{y}$ to indicate predicted values for the target variable $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9dce52",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Let's define the input and target variables and **initialize the weights** (give some initial random values to the parameters of the neural network model).\n",
    "As for hyperparameters, we set the learning rate $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c54fe269",
   "metadata": {},
   "outputs": [],
   "source": [
    "## input vars\n",
    "x1 = 0.15\n",
    "x2 = 0.08\n",
    "\n",
    "## training outputs (two continuous measurements -e.g. phenotypes- on the same example)\n",
    "y1 = 0.05\n",
    "y2 = 0.95\n",
    "\n",
    "## initialization of model parameters\n",
    "w1 = 0.15\n",
    "w2 = 0.10\n",
    "w3 = 0.12\n",
    "w4 = 0.09\n",
    "w5 = 0.18\n",
    "w6 = 0.20\n",
    "w7 = 0.16\n",
    "w8 = 0.24\n",
    "\n",
    "## setting the hyperparameters\n",
    "alpha = 0.75 ## learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a1de4",
   "metadata": {},
   "source": [
    "### A little set-up\n",
    "\n",
    "We import some needed Python libraries and define functions that we'll use in this example:\n",
    "\n",
    "- **logistic (sigmoid) function**: used to activate the linear combinations ($z$'s) calculated in the intermediate and output layers\n",
    "- **mean squared error (MSE)**: cost function used to evaluate the performance of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5656ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "## vectors\n",
    "y = np.array([y1,y2]) ## vector of training target variables\n",
    "\n",
    "## logistic function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "## cost function (mean squared error); the 1/2 is the normalization constant (2 outputs)\n",
    "def MSE(y,y_hat):\n",
    "    return sum((1/2)*(y-y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf60c46",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "We start moving forward along the network: from inputs to outputs.\n",
    "First, we calculate the linear combinations of inputs $z$'s in the hidden layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dcd58823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_11 is:  0.0305\n",
      "z_12 is:  0.0252\n"
     ]
    }
   ],
   "source": [
    "z11 = w1*x1 + w2*x2\n",
    "z12 = w3*x1 + w4*x2\n",
    "\n",
    "print('z_11 is: ', z11)\n",
    "print('z_12 is: ', z12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54f6d2",
   "metadata": {},
   "source": [
    "From the linear combinations $z$'s, we can now calculate the \"**activated**\" values $a$'s using the **sigmoid activation function**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9422e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_11 is:  0.5076244089586275\n",
      "a_12 is:  0.5062996666251706\n"
     ]
    }
   ],
   "source": [
    "a11 = sigmoid(z11)\n",
    "a12 = sigmoid(z12)\n",
    "\n",
    "print('a_11 is: ', a11)\n",
    "print('a_12 is: ', a12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a0a5c",
   "metadata": {},
   "source": [
    "We now move to the next layer, the **output layer** and calculate the linear combination of the inputs (the activations from the previous -hidden- layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc1a1285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_21 is:  0.19263232693758708\n",
      "z_22 is:  0.20273182542342133\n"
     ]
    }
   ],
   "source": [
    "z21 = w5*a11+w6*a12 ## linear combination from the first node in the output layer\n",
    "z22 = w7*a11+w8*a12 ## linear combination from the second node in the output layer\n",
    "\n",
    "print('z_21 is: ', z21)\n",
    "print('z_22 is: ', z22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc32651",
   "metadata": {},
   "source": [
    "We can now conclude the first forward propagation pass and calculate the predicted output values, using again the *sigmoid activation function*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "330af3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two predicted outputs are\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.54801; 0.55051'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat1 = sigmoid(z21)\n",
    "y_hat2 = sigmoid(z22)\n",
    "\n",
    "y_hat = np.array([y_hat1,y_hat2])\n",
    "\n",
    "print(\"The two predicted outputs are\")\n",
    "'; '.join([str(round(x,5)) for x in y_hat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1b123",
   "metadata": {},
   "source": [
    "### The cost\n",
    "\n",
    "With the predicted and actual target values we can calculate the cost, i.e. by how much we are currently off.\n",
    "Since in this simplified example we assumed that the target values are continuous variables, we can use **mean squared error** (*MSE*), a typical cost function for continuous variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6ea59df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two target outputs are\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.05; 0.95'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The two target outputs are\")\n",
    "'; '.join([str(round(x,5)) for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e4f4590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after the first forward propagation pass is:  0.20380293722737602\n"
     ]
    }
   ],
   "source": [
    "cost_1 = MSE(y,y_hat)\n",
    "\n",
    "print('The cost after the first forward propagation pass is: ', cost_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf933089",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "Finally, we can start moving backwards across the network: from the **cost** to the **coefficients**.\n",
    "\n",
    "Our goal with back propagation is to **update the weights** in the network so that they cause the network output to be closer the target output, thereby minimizing the error for each output neuron and the network as a whole.\n",
    "\n",
    "Let's start from the coefficient used to weigh the first input ($a_{11}$) in the linear combination $z_{21}$ calculated in the first node of the second (output) layer: $w_5$.\n",
    "\n",
    "We see from the figure below that we need to move backwards, from the cost, to the prediction $\\hat{y_1}$, to the lienar combination $z_{21}$. This is because $z_{21}$ is where the weight $w_5$ is indeed used:\n",
    "\n",
    "$$\n",
    "z_{21} = w_5 \\cdot a_{11} + w_6 \\cdot a_{12}\n",
    "$$\n",
    "\n",
    "Therefore: $MSE(y,\\hat{y})$ $\\rightarrow$ $\\hat{y_1}$ $\\rightarrow$ $z_{21}$ $\\rightarrow$ $w_5$\n",
    "\n",
    "<img src=\"first_back_prop.jpg\" alt=\"first back propgation step\" style=\"width: 800px;\"/>\n",
    "\n",
    "This very first backpropagation step involves calculating the **derivative of the cost function** $MSE()$ with respect to the target coefficient $w_5$. Using the [chain rule](https://mathsathome.com/chain-rule-differentiation/), this can be expressed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial w_5} = \\frac{\\partial MSE}{\\partial \\hat{y_1}} \\cdot \\frac{\\partial \\hat{y_1}}{\\partial z_{}\n",
    "$$\n",
    "\n",
    "dCost/d(y_hat1) * d(y_hat1)/d(h3) * d(h3)/d(w5) ## chain rule"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
