{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68f30817",
   "metadata": {},
   "source": [
    "## Backpropagation example\n",
    "\n",
    "With this numerical example we illustrate how forward- and back-propagation work to learn the weights (parameters) of a **neural network**.\n",
    "\n",
    "We use a very simple network: this is far from a real-application network, but serves well the purpose of showing with numbers what happens in the network.\n",
    "\n",
    "In particular, the following simplifications apply:\n",
    "- only one training example (one record, with two features $x_1$ and $x_2$)\n",
    "- two numerical outputs $y = [y_1,y_2]$ (e.g. two continuous measurements on the same training example)\n",
    "- one hidden (intermediate) layer with two nodes\n",
    "- no bias terms\n",
    "\n",
    "<img src=\"simple_network.jpg\" alt=\"network\" style=\"width: 800px;\"/>\n",
    "\n",
    "We use the notation:\n",
    "- $z_{ij}$ to indicate the linear combination of inputs in layer $i$ and node $j$\n",
    "- $a_{ij}$ to indicate the activated linear combination $z$ in layer $i$ and node $j$\n",
    "- $\\hat{y}$ to indicate predicted values for the target variable $y$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9dce52",
   "metadata": {},
   "source": [
    "### Initialization\n",
    "\n",
    "Let's define the input and target variables and **initialize the weights** (give some initial random values to the parameters of the neural network model).\n",
    "As for hyperparameters, we set the learning rate $\\alpha$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c54fe269",
   "metadata": {},
   "outputs": [],
   "source": [
    "## input vars\n",
    "x1 = 0.15\n",
    "x2 = 0.08\n",
    "\n",
    "## training outputs (two continuous measurements -e.g. phenotypes- on the same example)\n",
    "y1 = 0.05\n",
    "y2 = 0.95\n",
    "\n",
    "## initialization of model parameters\n",
    "w1 = 0.15\n",
    "w2 = 0.10\n",
    "w3 = 0.12\n",
    "w4 = 0.09\n",
    "w5 = 0.18\n",
    "w6 = 0.20\n",
    "w7 = 0.16\n",
    "w8 = 0.24\n",
    "\n",
    "## setting the hyperparameters\n",
    "alpha = 0.75 ## learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "061a1de4",
   "metadata": {},
   "source": [
    "### A little set-up\n",
    "\n",
    "We import some needed Python libraries and define functions that we'll use in this example:\n",
    "\n",
    "- **logistic (sigmoid) function**: used to activate the linear combinations ($z$'s) calculated in the intermediate and output layers\n",
    "- **mean squared error (MSE)**: cost function used to evaluate the performance of the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5656ad8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "\n",
    "## vectors\n",
    "y = np.array([y1,y2]) ## vector of training target variables\n",
    "\n",
    "## logistic function\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "## cost function (mean squared error); the 1/2 is the normalization constant (2 outputs)\n",
    "def MSE(y,y_hat):\n",
    "    return sum((1/2)*(y-y_hat)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf60c46",
   "metadata": {},
   "source": [
    "### Forward propagation\n",
    "\n",
    "We start moving forward along the network: from inputs to outputs.\n",
    "First, we calculate the linear combinations of inputs $z$'s in the hidden layer: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcd58823",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_11 is:  0.0305\n",
      "z_12 is:  0.0252\n"
     ]
    }
   ],
   "source": [
    "z11 = w1*x1 + w2*x2\n",
    "z12 = w3*x1 + w4*x2\n",
    "\n",
    "print('z_11 is: ', z11)\n",
    "print('z_12 is: ', z12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be54f6d2",
   "metadata": {},
   "source": [
    "From the linear combinations $z$'s, we can now calculate the \"**activated**\" values $a$'s using the **sigmoid activation function**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9422e30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a_11 is:  0.5076244089586275\n",
      "a_12 is:  0.5062996666251706\n"
     ]
    }
   ],
   "source": [
    "a11 = sigmoid(z11)\n",
    "a12 = sigmoid(z12)\n",
    "\n",
    "print('a_11 is: ', a11)\n",
    "print('a_12 is: ', a12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b38a0a5c",
   "metadata": {},
   "source": [
    "We now move to the next layer, the **output layer** and calculate the linear combination of the inputs (the activations from the previous -hidden- layer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc1a1285",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_21 is:  0.19263232693758708\n",
      "z_22 is:  0.20273182542342133\n"
     ]
    }
   ],
   "source": [
    "z21 = w5*a11+w6*a12 ## linear combination from the first node in the output layer\n",
    "z22 = w7*a11+w8*a12 ## linear combination from the second node in the output layer\n",
    "\n",
    "print('z_21 is: ', z21)\n",
    "print('z_22 is: ', z22)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc32651",
   "metadata": {},
   "source": [
    "We can now conclude the first forward propagation pass and calculate the predicted output values, using again the *sigmoid activation function*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "330af3f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two predicted outputs are\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.54801; 0.55051'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat1 = sigmoid(z21)\n",
    "y_hat2 = sigmoid(z22)\n",
    "\n",
    "y_hat = np.array([y_hat1,y_hat2])\n",
    "\n",
    "print(\"The two predicted outputs are\")\n",
    "'; '.join([str(round(x,5)) for x in y_hat])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b1b123",
   "metadata": {},
   "source": [
    "### The cost\n",
    "\n",
    "With the predicted and actual target values we can calculate the cost, i.e. by how much we are currently off.\n",
    "Since in this simplified example we assumed that the target values are continuous variables, we can use **mean squared error** (*MSE*), a typical cost function for continuous variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ea59df0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The two target outputs are\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.05; 0.95'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"The two target outputs are\")\n",
    "'; '.join([str(round(x,5)) for x in y])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e4f4590e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cost after the first forward propagation pass is:  0.20380293722737602\n"
     ]
    }
   ],
   "source": [
    "cost_1 = MSE(y,y_hat)\n",
    "\n",
    "print('The cost after the first forward propagation pass is: ', cost_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf933089",
   "metadata": {},
   "source": [
    "### Backpropagation\n",
    "\n",
    "Finally, we can start moving backwards across the network: from the **cost** to the **coefficients**.\n",
    "\n",
    "Our goal with back propagation is to **update the weights** in the network so that they cause the network output to be closer the target output, thereby minimizing the error for each output neuron and the network as a whole.\n",
    "\n",
    "Let's start from the coefficient used to weigh the first input ($a_{11}$) in the linear combination $z_{21}$ calculated in the first node of the second (output) layer: $w_5$.\n",
    "\n",
    "We see from the figure below that we need to move backwards, from the cost, to the prediction $\\hat{y}_1$, to the lienar combination $z_{21}$. This is because $z_{21}$ is where the weight $w_5$ is indeed used:\n",
    "\n",
    "$$\n",
    "z_{21} = w_5 \\cdot a_{11} + w_6 \\cdot a_{12}\n",
    "$$\n",
    "\n",
    "Therefore: $MSE(y,\\hat{y})$ $\\rightarrow$ $\\hat{y}_1$ $\\rightarrow$ $z_{21}$ $\\rightarrow$ $w_5$\n",
    "\n",
    "<img src=\"first_back_prop.jpg\" alt=\"first back propgation step\" style=\"width: 800px;\"/>\n",
    "\n",
    "This very first backpropagation step involves calculating the **derivative of the cost function** $MSE()$ with respect to the target coefficient $w_5$. Using the [chain rule](https://mathsathome.com/chain-rule-differentiation/), this can be expressed as:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial w_5} = \\frac{\\partial MSE}{\\partial \\hat{y}_1} \\cdot \\frac{\\partial \\hat{y}_1}{\\partial z_{21}} \\cdot \\frac{\\partial z_{21}}{\\partial w_5}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bd154bb",
   "metadata": {},
   "source": [
    "#### The first link in the chain\n",
    "\n",
    "Let's start with the first element in the chain: $ \\frac{\\partial MSE}{\\partial \\hat{y}_1} $.\n",
    "The mean squared error (cost) is the normalised sum of the two prediction errors ($\\hat{y}_1$ and $\\hat{y}_2$):\n",
    "\n",
    "$$\n",
    "\\frac{1}{2} (y_1-\\hat{y}_1)^2 + \\frac{1}{2}(y_2-\\hat{y}_2)^2\n",
    "$$\n",
    "\n",
    "since we are partialling on $\\hat{y}_1$ (our variable of interest), the second term in the sum is a **constant** $\\rightarrow$ the derivative of a constant is 0. We only need to take the derivative of the first term, of the form $f(x) = (a-x)^2$ $\\rightarrow$ $f'(x) = 2(a-x) \\cdot -1$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial \\hat{y}_1} = 2 \\cdot \\frac{1}{2} (y_1-\\hat{y}_1) \\cdot -1 + 0 = -(y_1-\\hat{y}_1)\n",
    "$$\n",
    "\n",
    "The $-1$ is because the variable in this partial derivative is $\\hat{y}_1$ (with coefficient -1):\n",
    "\n",
    "- $-1$: coefficient of -y_hat1 (function of function)\n",
    "- 0: derivative of a constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "352230a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first derivative in the chain:  0.49800971457469184\n"
     ]
    }
   ],
   "source": [
    "dMSEdy_hat1 = 2*(1/2)*(y1-y_hat1)*-1 + 0\n",
    "print('first derivative in the chain: ', dMSEdy_hat1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b47d0b",
   "metadata": {},
   "source": [
    "#### The second link in the chain\n",
    "\n",
    "Now we need to calculate the derivative of the prediction with respect to the linear corresponding combination, $\\frac{\\partial \\hat{y}_1}{ \\partial z_{21}}$. \n",
    "Let's remember that $\\hat{y}_1 = \\sigma(z_{21})$.\n",
    "\n",
    "The **derivative of the logistic (sigmoid) function** is: $\\sigma'(x) = \\sigma(x) \\cdot (1-sigma(x))$. \n",
    "In our case, this would be:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\hat{y}_1}{ \\partial z_{21}} = \\hat{y}_1 \\cdot (1 - \\hat{y}_1)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e76b3c60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "second derivative in the chain:  0.24769506730645663\n"
     ]
    }
   ],
   "source": [
    "dy_hat1dz21 = y_hat1*(1-y_hat1)\n",
    "print('second derivative in the chain: ', dy_hat1dz21)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74522a89",
   "metadata": {},
   "source": [
    "#### The final link in the chain\n",
    "\n",
    "Now it's the turn for $\\frac{\\partial z_{21}}{\\partial w_5}$. This is the derviative of the linear combination $z_{21} = w_5 \\cdot a_{11} + w_6 \\cdot a_{12}$, which is pretty simple: the coefficient of our variable of interest ($w_5$), i.e. $a_{11}$ (the remaining terms of the expression are constants, whose derivatives are zero).\n",
    "\n",
    "\n",
    "$$\n",
    "\\frac{\\partial z_{21}}{\\partial w_5} = a_1 + 0 + 0\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8583e77c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "third derivative in the chain:  0.5076244089586275\n"
     ]
    }
   ],
   "source": [
    "dz21dw5 = a11\n",
    "print('third derivative in the chain: ', dz21dw5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7623d225",
   "metadata": {},
   "source": [
    "#### Applying the chain rule\n",
    "\n",
    "We now have all the elements to calculate the product in the chain:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial MSE}{\\partial w_5} = \\frac{\\partial MSE}{\\partial \\hat{y}_1} \\cdot \\frac{\\partial \\hat{y}_1}{\\partial z_{21}} \\cdot \\frac{\\partial z_{21}}{\\partial w_5}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cf3bb7e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The derivative of the cost function with respect to w5 is:  0.06261778041978408\n"
     ]
    }
   ],
   "source": [
    "dMSEdw5 = dMSEdy_hat1*dy_hat1dz21*dz21dw5\n",
    "print('The derivative of the cost function with respect to w5 is: ', dMSEdw5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb06f84",
   "metadata": {},
   "source": [
    "### The updating step\n",
    "\n",
    "With the derivative of the cost function with respect to the coefficient $w_5$, we can now proceed to the updating of the coefficient (**the \"learning\"**):\n",
    "\n",
    "$$\n",
    "w_5+ = w_5 - \\alpha \\cdot \\frac{\\partial MSE}{\\partial w_5}\n",
    "$$\n",
    "\n",
    "where $\\alpha$ is the l"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
